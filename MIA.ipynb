{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2YNJUdWD3aLDsULl2Pjlk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/layafakher/Membership-Inference-Attack-on-LSTM-Model-/blob/main/MIA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyD_st68GeTc",
        "outputId": "e6b211d1-0b66-43d4-d08a-e9ff70bcaedd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mia\n",
            "  Downloading mia-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mia) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from mia) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from mia) (1.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from mia) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from mia) (4.66.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->mia) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->mia) (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->mia) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->mia) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->mia) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->mia) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->mia) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->mia) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->mia) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->mia) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->mia) (1.3.0)\n",
            "Building wheels for collected packages: mia\n",
            "  Building wheel for mia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mia: filename=mia-0.1.2-py3-none-any.whl size=11088 sha256=0976a8abd0fb89a3c4ff299763659c75d323f091f11307b942bf8b51bf08edd5\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/09/07/7a6ca0f72244f95e0a2363efb62231507a6bf32d7e111a86d7\n",
            "Successfully built mia\n",
            "Installing collected packages: mia\n",
            "Successfully installed mia-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-privacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvXWLMiuG0mk",
        "outputId": "cd7f522c-af92-47a4-98ef-d13ebacb2385"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-privacy\n",
            "  Downloading tensorflow_privacy-0.8.12-py3-none-any.whl (405 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m405.5/405.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py==1.*,>=1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-privacy) (1.4.0)\n",
            "Requirement already satisfied: attrs>=21.4 in /usr/local/lib/python3.10/dist-packages (from tensorflow-privacy) (23.1.0)\n",
            "Requirement already satisfied: dm-tree==0.1.8 in /usr/local/lib/python3.10/dist-packages (from tensorflow-privacy) (0.1.8)\n",
            "Collecting dp-accounting==0.4.3 (from tensorflow-privacy)\n",
            "  Downloading dp_accounting-0.4.3-py3-none-any.whl (104 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.8/104.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting immutabledict~=2.2 (from tensorflow-privacy)\n",
            "  Downloading immutabledict-2.2.5-py3-none-any.whl (4.1 kB)\n",
            "Requirement already satisfied: matplotlib~=3.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-privacy) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow-privacy) (1.23.5)\n",
            "Collecting packaging~=22.0 (from tensorflow-privacy)\n",
            "  Downloading packaging-22.0-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas~=1.4 in /usr/local/lib/python3.10/dist-packages (from tensorflow-privacy) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn==1.*,>=1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-privacy) (1.2.2)\n",
            "Requirement already satisfied: scipy~=1.9 in /usr/local/lib/python3.10/dist-packages (from tensorflow-privacy) (1.11.4)\n",
            "Requirement already satisfied: statsmodels~=0.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow-privacy) (0.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator~=2.4 in /usr/local/lib/python3.10/dist-packages (from tensorflow-privacy) (2.15.0)\n",
            "Requirement already satisfied: tensorflow-probability~=0.22.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-privacy) (0.22.0)\n",
            "Requirement already satisfied: tensorflow~=2.4 in /usr/local/lib/python3.10/dist-packages (from tensorflow-privacy) (2.15.0)\n",
            "Collecting tf-models-official~=2.13 (from tensorflow-privacy)\n",
            "  Downloading tf_models_official-2.15.0-py2.py3-none-any.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath~=1.2 in /usr/local/lib/python3.10/dist-packages (from dp-accounting==0.4.3->tensorflow-privacy) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.*,>=1.0->tensorflow-privacy) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.*,>=1.0->tensorflow-privacy) (3.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.3->tensorflow-privacy) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.3->tensorflow-privacy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.3->tensorflow-privacy) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.3->tensorflow-privacy) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.3->tensorflow-privacy) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.3->tensorflow-privacy) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.3->tensorflow-privacy) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas~=1.4->tensorflow-privacy) (2023.3.post1)\n",
            "Requirement already satisfied: patsy>=0.5.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels~=0.13->tensorflow-privacy) (0.5.4)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.4->tensorflow-privacy) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.4->tensorflow-privacy) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.4->tensorflow-privacy) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.4->tensorflow-privacy) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.4->tensorflow-privacy) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.4->tensorflow-privacy) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.4->tensorflow-privacy) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.4->tensorflow-privacy) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.4->tensorflow-privacy) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.4->tensorflow-privacy) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.4->tensorflow-privacy) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.4->tensorflow-privacy) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.4->tensorflow-privacy) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.4->tensorflow-privacy) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.4->tensorflow-privacy) (0.35.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.4->tensorflow-privacy) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.4->tensorflow-privacy) (2.15.1)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.4->tensorflow-privacy) (2.15.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.22.0->tensorflow-privacy) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.22.0->tensorflow-privacy) (2.2.1)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from tf-models-official~=2.13->tensorflow-privacy) (3.0.7)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.10/dist-packages (from tf-models-official~=2.13->tensorflow-privacy) (0.5.0)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.10/dist-packages (from tf-models-official~=2.13->tensorflow-privacy) (2.84.0)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.10/dist-packages (from tf-models-official~=2.13->tensorflow-privacy) (1.5.16)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (from tf-models-official~=2.13->tensorflow-privacy) (4.1.3)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from tf-models-official~=2.13->tensorflow-privacy) (4.8.1.78)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-models-official~=2.13->tensorflow-privacy) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official~=2.13->tensorflow-privacy) (9.0.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from tf-models-official~=2.13->tensorflow-privacy) (2.0.7)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official~=2.13->tensorflow-privacy) (6.0.1)\n",
            "Collecting sacrebleu (from tf-models-official~=2.13->tensorflow-privacy)\n",
            "  Downloading sacrebleu-2.4.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece (from tf-models-official~=2.13->tensorflow-privacy)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting seqeval (from tf-models-official~=2.13->tensorflow-privacy)\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from tf-models-official~=2.13->tensorflow-privacy) (4.9.4)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official~=2.13->tensorflow-privacy) (0.15.0)\n",
            "Collecting tensorflow-model-optimization>=0.4.1 (from tf-models-official~=2.13->tensorflow-privacy)\n",
            "  Downloading tensorflow_model_optimization-0.7.5-py2.py3-none-any.whl (241 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-text~=2.15.0 (from tf-models-official~=2.13->tensorflow-privacy)\n",
            "  Downloading tensorflow_text-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tf-slim>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official~=2.13->tensorflow-privacy) (1.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.4->tensorflow-privacy) (0.42.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official~=2.13->tensorflow-privacy) (0.22.0)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official~=2.13->tensorflow-privacy) (2.17.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official~=2.13->tensorflow-privacy) (0.1.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official~=2.13->tensorflow-privacy) (2.11.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official~=2.13->tensorflow-privacy) (4.1.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official~=2.13->tensorflow-privacy) (2023.11.17)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official~=2.13->tensorflow-privacy) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official~=2.13->tensorflow-privacy) (4.66.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official~=2.13->tensorflow-privacy) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official~=2.13->tensorflow-privacy) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official~=2.13->tensorflow-privacy) (6.1.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.4->tensorflow-privacy) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.4->tensorflow-privacy) (3.5.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.4->tensorflow-privacy) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.4->tensorflow-privacy) (3.0.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official~=2.13->tensorflow-privacy) (0.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official~=2.13->tensorflow-privacy) (0.3.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official~=2.13->tensorflow-privacy) (4.9)\n",
            "Collecting portalocker (from sacrebleu->tf-models-official~=2.13->tensorflow-privacy)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official~=2.13->tensorflow-privacy) (2023.6.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official~=2.13->tensorflow-privacy) (0.9.0)\n",
            "Collecting colorama (from sacrebleu->tf-models-official~=2.13->tensorflow-privacy)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official~=2.13->tensorflow-privacy) (4.9.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official~=2.13->tensorflow-privacy) (8.1.7)\n",
            "Requirement already satisfied: etils[enp,epath,etree]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official~=2.13->tensorflow-privacy) (1.6.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official~=2.13->tensorflow-privacy) (2.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official~=2.13->tensorflow-privacy) (1.14.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official~=2.13->tensorflow-privacy) (0.10.2)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official~=2.13->tensorflow-privacy) (0.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official~=2.13->tensorflow-privacy) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official~=2.13->tensorflow-privacy) (6.1.1)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official~=2.13->tensorflow-privacy) (3.17.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official~=2.13->tensorflow-privacy) (1.62.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official~=2.13->tensorflow-privacy) (5.3.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.4->tensorflow-privacy) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle>=1.3.9->tf-models-official~=2.13->tensorflow-privacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle>=1.3.9->tf-models-official~=2.13->tensorflow-privacy) (3.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow~=2.4->tensorflow-privacy) (2.1.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle>=1.3.9->tf-models-official~=2.13->tensorflow-privacy) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official~=2.13->tensorflow-privacy) (1.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.4->tensorflow-privacy) (3.2.2)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=44c7f3bda177402738e7f1f3fb83e90a68ee424ee8a1605986d66ba8eb030a87\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: sentencepiece, tensorflow-model-optimization, portalocker, packaging, immutabledict, colorama, sacrebleu, dp-accounting, seqeval, tensorflow-text, tf-models-official, tensorflow-privacy\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 23.2\n",
            "    Uninstalling packaging-23.2:\n",
            "      Successfully uninstalled packaging-23.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed colorama-0.4.6 dp-accounting-0.4.3 immutabledict-2.2.5 packaging-22.0 portalocker-2.8.2 sacrebleu-2.4.0 sentencepiece-0.1.99 seqeval-1.2.2 tensorflow-model-optimization-0.7.5 tensorflow-privacy-0.8.12 tensorflow-text-2.15.0 tf-models-official-2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rasa==1.1.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7kghtIGHDMc",
        "outputId": "90374d87-f7e7-4016-c49e-cdc7f4e9a07a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rasa==1.1.4\n",
            "  Downloading rasa-1.1.4-py3-none-any.whl (447 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.2/447.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests~=2.22 in /usr/local/lib/python3.10/dist-packages (from rasa==1.1.4) (2.31.0)\n",
            "Collecting boto3~=1.9 (from rasa==1.1.4)\n",
            "  Downloading boto3-1.34.14-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from rasa==1.1.4) (3.7.1)\n",
            "Collecting simplejson~=3.16 (from rasa==1.1.4)\n",
            "  Downloading simplejson-3.19.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=18 in /usr/local/lib/python3.10/dist-packages (from rasa==1.1.4) (23.1.0)\n",
            "Collecting jsonpickle~=1.1 (from rasa==1.1.4)\n",
            "  Downloading jsonpickle-1.5.2-py2.py3-none-any.whl (37 kB)\n",
            "Collecting redis~=3.2 (from rasa==1.1.4)\n",
            "  Downloading redis-3.5.3-py2.py3-none-any.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.1/72.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fakeredis~=1.0 (from rasa==1.1.4)\n",
            "  Downloading fakeredis-1.10.2-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.0/43.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymongo~=3.8 (from rasa==1.1.4)\n",
            "  Downloading pymongo-3.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (516 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.2/516.2 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy~=1.16 in /usr/local/lib/python3.10/dist-packages (from rasa==1.1.4) (1.23.5)\n",
            "Requirement already satisfied: scipy~=1.2 in /usr/local/lib/python3.10/dist-packages (from rasa==1.1.4) (1.11.4)\n",
            "INFO: pip is looking at multiple versions of rasa to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Ignored the following versions that require a different python version: 1.10.10 Requires-Python >=3.6,<3.8; 1.10.11 Requires-Python >=3.6,<3.8; 1.10.12 Requires-Python >=3.6,<3.8; 1.10.13 Requires-Python >=3.6,<3.8; 1.10.14 Requires-Python >=3.6,<3.8; 1.10.15 Requires-Python >=3.6,<3.8; 1.10.16 Requires-Python >=3.6,<3.8; 1.10.17 Requires-Python >=3.6,<3.8; 1.10.18 Requires-Python >=3.6,<3.8; 1.10.19 Requires-Python >=3.6,<3.8; 1.10.20 Requires-Python >=3.6,<3.8; 1.10.21 Requires-Python >=3.6,<3.8; 1.10.22 Requires-Python >=3.6,<3.8; 1.10.23 Requires-Python >=3.6,<3.8; 1.10.24 Requires-Python >=3.6,<3.8; 1.10.25 Requires-Python >=3.6,<3.8; 1.10.26 Requires-Python >=3.6,<3.8; 1.10.3 Requires-Python >=3.6,<3.8; 1.10.4 Requires-Python >=3.6,<3.8; 1.10.5 Requires-Python >=3.6,<3.8; 1.10.6 Requires-Python >=3.6,<3.8; 1.10.7 Requires-Python >=3.6,<3.8; 1.10.8 Requires-Python >=3.6,<3.8; 1.10.9 Requires-Python >=3.6,<3.8; 2.0.0 Requires-Python >=3.6,<3.9; 2.0.0a1 Requires-Python >=3.6,<3.9; 2.0.0a2 Requires-Python >=3.6,<3.9; 2.0.0a3 Requires-Python >=3.6,<3.9; 2.0.0a4 Requires-Python >=3.6,<3.9; 2.0.0a5 Requires-Python >=3.6,<3.9; 2.0.0a6 Requires-Python >=3.6,<3.9; 2.0.0rc1 Requires-Python >=3.6,<3.9; 2.0.0rc2 Requires-Python >=3.6,<3.9; 2.0.0rc3 Requires-Python >=3.6,<3.9; 2.0.0rc4 Requires-Python >=3.6,<3.9; 2.0.1 Requires-Python >=3.6,<3.9; 2.0.2 Requires-Python >=3.6,<3.9; 2.0.3 Requires-Python >=3.6,<3.9; 2.0.4 Requires-Python >=3.6,<3.9; 2.0.5 Requires-Python >=3.6,<3.9; 2.0.6 Requires-Python >=3.6,<3.9; 2.0.7 Requires-Python >=3.6,<3.9; 2.0.8 Requires-Python >=3.6,<3.9; 2.1.0 Requires-Python >=3.6,<3.9; 2.1.1 Requires-Python >=3.6,<3.9; 2.1.2 Requires-Python >=3.6,<3.9; 2.1.3 Requires-Python >=3.6,<3.9; 2.2.0 Requires-Python >=3.6,<3.9; 2.2.0a1 Requires-Python >=3.6,<3.9; 2.2.1 Requires-Python >=3.6,<3.9; 2.2.10 Requires-Python >=3.6,<3.9; 2.2.2 Requires-Python >=3.6,<3.9; 2.2.3 Requires-Python >=3.6,<3.9; 2.2.4 Requires-Python >=3.6,<3.9; 2.2.5 Requires-Python >=3.6,<3.9; 2.2.6 Requires-Python >=3.6,<3.9; 2.2.7 Requires-Python >=3.6,<3.9; 2.2.8 Requires-Python >=3.6,<3.9; 2.2.9 Requires-Python >=3.6,<3.9; 2.3.0 Requires-Python >=3.6,<3.9; 2.3.1 Requires-Python >=3.6,<3.9; 2.3.2 Requires-Python >=3.6,<3.9; 2.3.3 Requires-Python >=3.6,<3.9; 2.3.4 Requires-Python >=3.6,<3.9; 2.3.5 Requires-Python >=3.6,<3.9; 2.4.0 Requires-Python >=3.6,<3.9; 2.4.1 Requires-Python >=3.6,<3.9; 2.4.2 Requires-Python >=3.6,<3.9; 2.4.3 Requires-Python >=3.6,<3.9; 2.5.0 Requires-Python >=3.6,<3.9; 2.5.1 Requires-Python >=3.6,<3.9; 2.5.2 Requires-Python >=3.6,<3.9; 2.6.0 Requires-Python >=3.6,<3.9; 2.6.1 Requires-Python >=3.6,<3.9; 2.6.2 Requires-Python >=3.6,<3.9; 2.6.3 Requires-Python >=3.6,<3.9; 2.7.0 Requires-Python >=3.6,<3.9; 2.7.1 Requires-Python >=3.6,<3.9; 2.7.2 Requires-Python >=3.6,<3.9; 2.8.0 Requires-Python >=3.6,<3.9; 2.8.1 Requires-Python >=3.6,<3.9; 2.8.10 Requires-Python >=3.6,<3.9; 2.8.11 Requires-Python >=3.6,<3.9; 2.8.12 Requires-Python >=3.6,<3.9; 2.8.13 Requires-Python >=3.6,<3.9; 2.8.14 Requires-Python >=3.6,<3.9; 2.8.15 Requires-Python >=3.6,<3.9; 2.8.16 Requires-Python >=3.6,<3.9; 2.8.17 Requires-Python >=3.6,<3.9; 2.8.18 Requires-Python >=3.6,<3.9; 2.8.19 Requires-Python >=3.6,<3.9; 2.8.2 Requires-Python >=3.6,<3.9; 2.8.20 Requires-Python >=3.6,<3.9; 2.8.21 Requires-Python >=3.6,<3.9; 2.8.22 Requires-Python >=3.6,<3.9; 2.8.23 Requires-Python >=3.6,<3.9; 2.8.24 Requires-Python >=3.6,<3.9; 2.8.25 Requires-Python >=3.6,<3.9; 2.8.26 Requires-Python >=3.6,<3.9; 2.8.27 Requires-Python >=3.7,<3.9; 2.8.28 Requires-Python >=3.7,<3.9; 2.8.29 Requires-Python >=3.7,<3.9; 2.8.3 Requires-Python >=3.6,<3.9; 2.8.30 Requires-Python >=3.7,<3.9; 2.8.31 Requires-Python >=3.7,<3.9; 2.8.32 Requires-Python >=3.7,<3.9; 2.8.33 Requires-Python >=3.7,<3.9; 2.8.34 Requires-Python >=3.7,<3.9; 2.8.4 Requires-Python >=3.6,<3.9; 2.8.5 Requires-Python >=3.6,<3.9; 2.8.6 Requires-Python >=3.6,<3.9; 2.8.7 Requires-Python >=3.6,<3.9; 2.8.8 Requires-Python >=3.6,<3.9; 2.8.9 Requires-Python >=3.6,<3.9; 3.0.0 Requires-Python >=3.7,<3.9; 3.0.0rc1 Requires-Python >=3.7,<3.9; 3.0.0rc2 Requires-Python >=3.7,<3.9; 3.0.0rc3 Requires-Python >=3.7,<3.9; 3.0.1 Requires-Python >=3.7,<3.9; 3.0.11 Requires-Python >=3.7,<3.9; 3.0.12 Requires-Python >=3.7,<3.9; 3.0.13 Requires-Python >=3.7,<3.9; 3.0.2 Requires-Python >=3.7,<3.9; 3.0.3 Requires-Python >=3.7,<3.9; 3.0.4 Requires-Python >=3.7,<3.9; 3.0.5 Requires-Python >=3.7,<3.9; 3.0.6 Requires-Python >=3.7,<3.9; 3.0.7 Requires-Python >=3.7,<3.9; 3.0.8 Requires-Python >=3.7,<3.9; 3.0.9 Requires-Python >=3.7,<3.9; 3.1.0 Requires-Python >=3.7,<3.10; 3.1.1 Requires-Python >=3.7,<3.10; 3.1.2 Requires-Python >=3.7,<3.10; 3.1.3 Requires-Python >=3.7,<3.10; 3.1.4 Requires-Python >=3.7,<3.10; 3.1.5 Requires-Python >=3.7,<3.10; 3.1.6 Requires-Python >=3.7,<3.10; 3.1.7 Requires-Python >=3.7,<3.10; 3.2.0 Requires-Python >=3.7,<3.10; 3.2.1 Requires-Python >=3.7,<3.10; 3.2.10 Requires-Python >=3.7,<3.10; 3.2.11 Requires-Python >=3.7,<3.10; 3.2.12 Requires-Python >=3.7,<3.10; 3.2.13 Requires-Python >=3.7,<3.10; 3.2.2 Requires-Python >=3.7,<3.10; 3.2.4 Requires-Python >=3.7,<3.10; 3.2.5 Requires-Python >=3.7,<3.10; 3.2.6 Requires-Python >=3.7,<3.10; 3.2.7 Requires-Python >=3.7,<3.10; 3.2.8 Requires-Python >=3.7,<3.10; 3.3.0 Requires-Python >=3.7,<3.10; 3.3.0a1 Requires-Python >=3.7,<3.10; 3.3.1 Requires-Python >=3.7,<3.10; 3.3.10 Requires-Python >=3.7,<3.10; 3.3.11 Requires-Python >=3.7,<3.10; 3.3.12 Requires-Python >=3.7,<3.10; 3.3.2 Requires-Python >=3.7,<3.10; 3.3.3 Requires-Python >=3.7,<3.10; 3.3.4 Requires-Python >=3.7,<3.10; 3.3.5 Requires-Python >=3.7,<3.10; 3.3.6 Requires-Python >=3.7,<3.10; 3.3.7 Requires-Python >=3.7,<3.10; 3.3.8 Requires-Python >=3.7,<3.10; 3.3.9 Requires-Python >=3.7,<3.10\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow~=1.13.0 (from rasa) (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.0.post1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow~=1.13.0\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ucimlrepo\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwlOWrOuH05j",
        "outputId": "ec40f75c-c3ca-44ea-8257-f0e2fc42ea22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ucimlrepo\n",
            "  Downloading ucimlrepo-0.0.3-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: ucimlrepo\n",
            "Successfully installed ucimlrepo-0.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ucimlrepo import fetch_ucirepo"
      ],
      "metadata": {
        "id": "WveslTWZH5wU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D9N6o-zkGKXV",
        "outputId": "5e39e6b4-f499-44c9-ba7d-50c1d3afa781"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and splitting dataset\n",
            "       age         workclass  fnlwgt  education  education-num  \\\n",
            "0       39         State-gov   77516  Bachelors             13   \n",
            "1       50  Self-emp-not-inc   83311  Bachelors             13   \n",
            "2       38           Private  215646    HS-grad              9   \n",
            "3       53           Private  234721       11th              7   \n",
            "4       28           Private  338409  Bachelors             13   \n",
            "...    ...               ...     ...        ...            ...   \n",
            "48837   39           Private  215419  Bachelors             13   \n",
            "48838   64               NaN  321403    HS-grad              9   \n",
            "48839   38           Private  374983  Bachelors             13   \n",
            "48840   44           Private   83891  Bachelors             13   \n",
            "48841   35      Self-emp-inc  182148  Bachelors             13   \n",
            "\n",
            "           marital-status         occupation    relationship  \\\n",
            "0           Never-married       Adm-clerical   Not-in-family   \n",
            "1      Married-civ-spouse    Exec-managerial         Husband   \n",
            "2                Divorced  Handlers-cleaners   Not-in-family   \n",
            "3      Married-civ-spouse  Handlers-cleaners         Husband   \n",
            "4      Married-civ-spouse     Prof-specialty            Wife   \n",
            "...                   ...                ...             ...   \n",
            "48837            Divorced     Prof-specialty   Not-in-family   \n",
            "48838             Widowed                NaN  Other-relative   \n",
            "48839  Married-civ-spouse     Prof-specialty         Husband   \n",
            "48840            Divorced       Adm-clerical       Own-child   \n",
            "48841  Married-civ-spouse    Exec-managerial         Husband   \n",
            "\n",
            "                     race     sex  capital-gain  capital-loss  hours-per-week  \\\n",
            "0                   White    Male          2174             0              40   \n",
            "1                   White    Male             0             0              13   \n",
            "2                   White    Male             0             0              40   \n",
            "3                   Black    Male             0             0              40   \n",
            "4                   Black  Female             0             0              40   \n",
            "...                   ...     ...           ...           ...             ...   \n",
            "48837               White  Female             0             0              36   \n",
            "48838               Black    Male             0             0              40   \n",
            "48839               White    Male             0             0              50   \n",
            "48840  Asian-Pac-Islander    Male          5455             0              40   \n",
            "48841               White    Male             0             0              60   \n",
            "\n",
            "      native-country  Salary  \n",
            "0      United-States   <=50K  \n",
            "1      United-States   <=50K  \n",
            "2      United-States   <=50K  \n",
            "3      United-States   <=50K  \n",
            "4               Cuba   <=50K  \n",
            "...              ...     ...  \n",
            "48837  United-States  <=50K.  \n",
            "48838  United-States  <=50K.  \n",
            "48839  United-States  <=50K.  \n",
            "48840  United-States  <=50K.  \n",
            "48841  United-States   >50K.  \n",
            "\n",
            "[48842 rows x 15 columns]\n",
            "(48842, 15)\n",
            "Index(['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
            "       'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
            "       'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n",
            "       'Salary'],\n",
            "      dtype='object')\n",
            "    Age  Workclass    fnlwgt  Education  Education-num  Marital-status  \\\n",
            "0  39.0        7.0   77516.0        9.0           13.0             4.0   \n",
            "1  50.0        6.0   83311.0        9.0           13.0             2.0   \n",
            "2  38.0        4.0  215646.0       11.0            9.0             0.0   \n",
            "3  53.0        4.0  234721.0        1.0            7.0             2.0   \n",
            "4  28.0        4.0  338409.0        9.0           13.0             2.0   \n",
            "\n",
            "   Occupation  Relationship  Race  Sex  Capital-gain  Capital-loss  \\\n",
            "0         1.0           1.0   4.0  1.0        2174.0           0.0   \n",
            "1         4.0           0.0   4.0  1.0           0.0           0.0   \n",
            "2         6.0           1.0   4.0  1.0           0.0           0.0   \n",
            "3         6.0           0.0   2.0  1.0           0.0           0.0   \n",
            "4        10.0           5.0   2.0  0.0           0.0           0.0   \n",
            "\n",
            "   Hours-per-week  Native-country  Salary  \n",
            "0            40.0            39.0     0.0  \n",
            "1            13.0            39.0     0.0  \n",
            "2            40.0            39.0     0.0  \n",
            "3            40.0            39.0     0.0  \n",
            "4            40.0             5.0     0.0  \n",
            "Training the target model...\n",
            "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCc\n",
            "        Age  Workclass    fnlwgt  Education  Education-num  Marital-status  \\\n",
            "22954  34.0        4.0  351810.0        9.0           13.0             4.0   \n",
            "23928  50.0        4.0  151580.0        7.0           12.0             2.0   \n",
            "28601  44.0        4.0  194924.0       11.0            9.0             2.0   \n",
            "32341  74.0        6.0  199136.0        9.0           13.0             6.0   \n",
            "42270  25.0        4.0  144301.0       15.0           10.0             2.0   \n",
            "...     ...        ...       ...        ...            ...             ...   \n",
            "12203  26.0        4.0   34393.0       15.0           10.0             0.0   \n",
            "38105  30.0        4.0  157911.0       15.0           10.0             2.0   \n",
            "31638  50.0        4.0  344621.0       11.0            9.0             0.0   \n",
            "3247   64.0        5.0  132832.0       11.0            9.0             2.0   \n",
            "25542  27.0        2.0  332249.0       11.0            9.0             4.0   \n",
            "\n",
            "       Occupation  Relationship  Race  Sex  Capital-gain  Capital-loss  \\\n",
            "22954        10.0           1.0   4.0  0.0           0.0           0.0   \n",
            "23928         3.0           0.0   4.0  1.0           0.0           0.0   \n",
            "28601        12.0           0.0   4.0  1.0           0.0           0.0   \n",
            "32341         3.0           1.0   4.0  1.0       15831.0           0.0   \n",
            "42270         6.0           0.0   4.0  1.0           0.0           0.0   \n",
            "...           ...           ...   ...  ...           ...           ...   \n",
            "12203         1.0           1.0   4.0  0.0           0.0           0.0   \n",
            "38105         1.0           5.0   4.0  0.0           0.0           0.0   \n",
            "31638         7.0           1.0   4.0  0.0           0.0           0.0   \n",
            "3247          1.0           5.0   4.0  0.0       20051.0           0.0   \n",
            "25542         5.0           3.0   4.0  1.0           0.0           0.0   \n",
            "\n",
            "       Hours-per-week  Native-country  \n",
            "22954            45.0             5.0  \n",
            "23928            45.0            39.0  \n",
            "28601            35.0            39.0  \n",
            "32341             8.0            11.0  \n",
            "42270            30.0            39.0  \n",
            "...               ...             ...  \n",
            "12203            40.0            39.0  \n",
            "38105            40.0            39.0  \n",
            "31638            40.0            39.0  \n",
            "3247             40.0             0.0  \n",
            "25542            40.0            39.0  \n",
            "\n",
            "[19520 rows x 14 columns]\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 328, in __call__\n        total_total_loss_mean_value = tf.add_n(total_loss_mean_values)\n\n    ValueError: Shapes must be equal rank, but are 1 and 0\n    \tFrom merging shape 0 with other shapes. for '{{node AddN}} = AddN[N=2, T=DT_FLOAT](sparse_categorical_crossentropy/weighted_loss/Mul, dense_5/kernel/Regularizer/mul)' with input shapes: [32], [].\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-4a668ebbbf40>\u001b[0m in \u001b[0;36m<cell line: 248>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    271\u001b[0m                                                                    batch_size)\n\u001b[1;32m    272\u001b[0m     \u001b[0mfeature_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-4a668ebbbf40>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0mtm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     tm.fit(target_X_train.values,\n\u001b[0m\u001b[1;32m    197\u001b[0m            \u001b[0mtarget_y_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m            \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 328, in __call__\n        total_total_loss_mean_value = tf.add_n(total_loss_mean_values)\n\n    ValueError: Shapes must be equal rank, but are 1 and 0\n    \tFrom merging shape 0 with other shapes. for '{{node AddN}} = AddN[N=2, T=DT_FLOAT](sparse_categorical_crossentropy/weighted_loss/Mul, dense_5/kernel/Regularizer/mul)' with input shapes: [32], [].\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "path = ''\n",
        "\n",
        "# depent on tensorflow 1.14\n",
        "import tensorflow as tf\n",
        "from mia.estimators import ShadowModelBundle, prepare_attack_data\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.utils import resample\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.regularizers import l1\n",
        "# privacy package\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer\n",
        "\n",
        "\n",
        "\n",
        "import random\n",
        "random.seed(19122)\n",
        "\n",
        "\n",
        "np.random.seed(19122)\n",
        "\n",
        "\n",
        "GradientDescentOptimizer = tf.compat.v1.train.GradientDescentOptimizer\n",
        "\n",
        "def split_to_be_divisible(X, y, shadow_perc, batch_size):\n",
        "    \"\"\"\n",
        "    Split a dataframe into target dataset and shadow dataset, and make them divisible by batch size.\n",
        "\n",
        "    :param X: genotype data\n",
        "    :param y: phenotype data\n",
        "    :param shadow_perc: specified percent for shadow dataset, target_perc = 1 - shadow_perc\n",
        "    :param batch_size: batch_size for training process\n",
        "\n",
        "    :return: target datasets, shadow datasets\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    assert y.shape[0] == X.shape[0]\n",
        "\n",
        "\n",
        "    total_row = X.shape[0]\n",
        "    num_shadow_row = int(total_row * shadow_perc) - int(total_row * shadow_perc) % batch_size\n",
        "    num_target_row = (total_row - num_shadow_row) - (total_row - num_shadow_row) % batch_size\n",
        "\n",
        "    # split train and valid\n",
        "    random_row = np.random.permutation(total_row)\n",
        "    shadow_row = random_row[:num_shadow_row]\n",
        "    target_row = random_row[-num_target_row:]\n",
        "\n",
        "    target_X = X.iloc[target_row]\n",
        "    shadow_X = X.iloc[shadow_row]\n",
        "\n",
        "    target_y = y.iloc[target_row]\n",
        "    shadow_y = y.iloc[shadow_row]\n",
        "\n",
        "    return target_X, target_y, shadow_X, shadow_y\n",
        "\n",
        "\n",
        "def target_model():\n",
        "    \"\"\"The architecture of the target model.\n",
        "    The attack is white-box, hence the attacker is assumed to know this architecture too.\n",
        "\n",
        "    :return: target model\n",
        "    \"\"\"\n",
        "    classifier = tf.keras.Sequential([\n",
        "        tf.keras.Input((feature_size), name='feature'),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "        tf.keras.layers.Dense(2, activation=tf.nn.softmax, kernel_regularizer=l1(kernel_regularization))\n",
        "    ])\n",
        "\n",
        "    if dpsgd:\n",
        "        optimizer = DPGradientDescentGaussianOptimizer(\n",
        "            l2_norm_clip=l2_norm_clip,\n",
        "            noise_multiplier=noise_multiplier,\n",
        "            num_microbatches=int(microbatches_perc * batch_size),\n",
        "            learning_rate=learning_rate)\n",
        "\n",
        "\n",
        "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.compat.v2.losses.Reduction.NONE)\n",
        "    else:\n",
        "        optimizer = GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.compat.v2.losses.Reduction.NONE)\n",
        "\n",
        "\n",
        "    classifier.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "    return classifier\n",
        "\n",
        "\n",
        "def shadow_model():\n",
        "    \"\"\"The architecture of the shadow model is same as target model, because the attack is white-box,\n",
        "    hence the attacker is assumed to know this architecture too.\n",
        "\n",
        "    :return: shadow model\n",
        "    \"\"\"\n",
        "\n",
        "    classifier = Sequential()\n",
        "    classifier = tf.keras.Sequential([\n",
        "        tf.keras.Input((feature_size), name='feature'),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "        tf.keras.layers.Dense(2, activation=tf.nn.softmax,  kernel_regularizer=l1(kernel_regularization))\n",
        "    ])\n",
        "\n",
        "    if dpsgd:\n",
        "        optimizer = DPGradientDescentGaussianOptimizer(\n",
        "            l2_norm_clip=l2_norm_clip,\n",
        "            noise_multiplier=noise_multiplier,\n",
        "            num_microbatches=int(microbatches_perc * batch_size),\n",
        "            learning_rate=learning_rate)\n",
        "\n",
        "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.compat.v2.losses.Reduction.NONE)\n",
        "\n",
        "    else:\n",
        "        optimizer = GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.compat.v2.losses.Reduction.NONE)\n",
        "\n",
        "\n",
        "    classifier.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "    return classifier\n",
        "\n",
        "\n",
        "def  personal_income_classification():\n",
        "    \"\"\"\n",
        "        Load a dataset and encode categorical variables.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    df = fetch_ucirepo(id=2).data.features\n",
        "    df['Salary'] = fetch_ucirepo(id=2).data.targets\n",
        "\n",
        "    print(df)\n",
        "\n",
        "    print(df.shape)\n",
        "    print(df.columns)\n",
        "\n",
        "\n",
        "    df.columns = ['Age', 'Workclass', 'fnlwgt', 'Education', 'Education-num', 'Marital-status', 'Occupation',\n",
        "                  'Relationship', 'Race', 'Sex', 'Capital-gain', 'Capital-loss',\n",
        "                  'Hours-per-week', 'Native-country', 'Salary']\n",
        "\n",
        "    df['Age'] = df['Age'].astype(np.float32)\n",
        "\n",
        "    df['fnlwgt'] = df['fnlwgt'].astype(np.float32)\n",
        "    df['Education-num'] = df['Education-num'].astype(np.float32)\n",
        "    df['Capital-gain'] = df['Capital-gain'].astype(np.float32)\n",
        "    df['Capital-loss'] = df['Capital-loss'].astype(np.float32)\n",
        "    df['Hours-per-week'] = df['Hours-per-week'].astype(np.float32)\n",
        "\n",
        "    df['Workclass'] = df['Workclass'].astype('category').cat.codes.astype(np.float32)\n",
        "    df['Education'] = df['Education'].astype('category').cat.codes.astype(np.float32)\n",
        "    df['Marital-status'] = df['Marital-status'].astype('category').cat.codes.astype(np.float32)\n",
        "    df['Occupation'] = df['Occupation'].astype('category').cat.codes.astype(np.float32)\n",
        "    df['Relationship'] = df['Relationship'].astype('category').cat.codes.astype(np.float32)\n",
        "    df['Race'] = df['Race'].astype('category').cat.codes.astype(np.float32)\n",
        "    df['Sex'] = df['Sex'].astype('category').cat.codes.astype(np.float32)\n",
        "    df['Native-country'] = df['Native-country'].astype('category').cat.codes.astype(np.float32)\n",
        "    df['Salary'] = df['Salary'].astype('category').cat.codes.astype(np.float32)\n",
        "\n",
        "    df_train = df[\n",
        "        ['Age', 'Workclass', 'fnlwgt', 'Education', 'Education-num', 'Marital-status', 'Occupation', 'Relationship',\n",
        "         'Race', 'Sex', 'Capital-gain', 'Capital-loss',\n",
        "         'Hours-per-week', 'Native-country']]\n",
        "\n",
        "    df_test = df[['Salary']]\n",
        "\n",
        "    print(df.head())\n",
        "    return df_train, df_test\n",
        "\n",
        "def main():\n",
        "    print(\"Training the target model...\")\n",
        "\n",
        "\n",
        "    target_X_train, target_y_train, target_X_valid, target_y_valid = split_to_be_divisible(target_X,\n",
        "                                                                                           target_y,\n",
        "                                                                                           0.2,\n",
        "                                                                                      batch_size)\n",
        "\n",
        "    print(\"CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCc\")\n",
        "    print(target_X_train)\n",
        "\n",
        "    tm = target_model()\n",
        "    tm.fit(target_X_train.values,\n",
        "           target_y_train.values,\n",
        "           batch_size=batch_size,\n",
        "           epochs=epochs,\n",
        "           validation_data=[target_X_valid.values, target_y_valid.values],\n",
        "           verbose=1)\n",
        "\n",
        "    print(\"Training the shadow models.\")\n",
        "\n",
        "    SHADOW_DATASET_SIZE = int(shadow_X.shape[0] / 2)\n",
        "    smb = ShadowModelBundle(\n",
        "        shadow_model,\n",
        "        shadow_dataset_size=SHADOW_DATASET_SIZE,\n",
        "        num_models=1,\n",
        "    )\n",
        "\n",
        "    attacker_X, attacker_y = smb.fit_transform(shadow_X.values, shadow_y.values,\n",
        "                                               fit_kwargs=dict(epochs=epochs,\n",
        "                                                               batch_size=batch_size,\n",
        "                                                               verbose=1),\n",
        "                                               )\n",
        "\n",
        "    print(\"Training attack model...\")\n",
        "    clf = RandomForestClassifier(max_depth=2)\n",
        "    clf.fit(attacker_X, attacker_y)\n",
        "\n",
        "\n",
        "    ATTACK_TEST_DATASET_SIZE = unused_X.shape[0]\n",
        "\n",
        "    data_in = target_X_train[:ATTACK_TEST_DATASET_SIZE], target_y_train[:ATTACK_TEST_DATASET_SIZE]\n",
        "    data_out = unused_X[:ATTACK_TEST_DATASET_SIZE], unused_y[:ATTACK_TEST_DATASET_SIZE]\n",
        "\n",
        "\n",
        "    attack_test_data, real_membership_labels = prepare_attack_data(tm, data_in, data_out)\n",
        "\n",
        "    attack_guesses = clf.predict(attack_test_data)\n",
        "    attack_accuracy = np.mean(attack_guesses == real_membership_labels)\n",
        "    print('attack accuracy: {}'.format(attack_accuracy))\n",
        "    acc = accuracy_score(real_membership_labels, attack_guesses)\n",
        "    print('attack acc: {}'.format(acc))\n",
        "\n",
        "    prec =  precision_score(real_membership_labels, attack_guesses)\n",
        "    print('Precision: {}'.format(prec))\n",
        "\n",
        "    recall = recall_score(real_membership_labels, attack_guesses)\n",
        "    print('Recall: {}'.format(recall))\n",
        "\n",
        "    fscore = f1_score(real_membership_labels, attack_guesses)\n",
        "    print('F1-Score: {}'.format(fscore))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\n",
        "    dpsgd = True\n",
        "\n",
        "\n",
        "    epochs = 10\n",
        "    batch_size = 32\n",
        "    microbatches_perc = .5\n",
        "    learning_rate = 0.01\n",
        "    kernel_regularization = 1.2 #0.001352\n",
        "    noise_multiplier = 0.8\n",
        "    l2_norm_clip = 0.8 #1.0\n",
        "\n",
        "    print(\"Loading and splitting dataset\")\n",
        "\n",
        "    X, y = personal_income_classification()\n",
        "    target_X, target_y, shadow_X, shadow_y = split_to_be_divisible(X, y, 0.5, batch_size=80)\n",
        "\n",
        "\n",
        "    shadow_X, shadow_y, unused_X, unused_y = split_to_be_divisible(shadow_X,\n",
        "                                                                   shadow_y,\n",
        "                                                                   0.3,\n",
        "                                                                   batch_size)\n",
        "    feature_size = target_X.shape[1]\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.utils import resample\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.regularizers import l1\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(19122)\n",
        "\n",
        "GradientDescentOptimizer = tf.compat.v1.train.GradientDescentOptimizer\n",
        "\n",
        "def split_to_be_divisible(X, y, shadow_perc, batch_size):\n",
        "    assert y.shape[0] == X.shape[0]\n",
        "\n",
        "    total_row = X.shape[0]\n",
        "    num_shadow_row = int(total_row * shadow_perc) - int(total_row * shadow_perc) % batch_size\n",
        "    num_target_row = (total_row - num_shadow_row) - (total_row - num_shadow_row) % batch_size\n",
        "\n",
        "    random_row = np.random.permutation(total_row)\n",
        "    shadow_row = random_row[:num_shadow_row]\n",
        "    target_row = random_row[-num_target_row:]\n",
        "\n",
        "    target_X = X.iloc[target_row]\n",
        "    shadow_X = X.iloc[shadow_row]\n",
        "\n",
        "    target_y = y.iloc[target_row]\n",
        "    shadow_y = y.iloc[shadow_row]\n",
        "\n",
        "    return target_X, target_y, shadow_X, shadow_y\n",
        "\n",
        "def target_model(feature_size, dpsgd, l2_norm_clip, noise_multiplier, microbatches_perc, learning_rate, kernel_regularization):\n",
        "    classifier = tf.keras.Sequential([\n",
        "        tf.keras.Input((feature_size), name='feature'),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "        tf.keras.layers.Dense(2, activation=tf.nn.softmax, kernel_regularizer=l1(kernel_regularization))\n",
        "    ])\n",
        "\n",
        "    if dpsgd:\n",
        "        optimizer = DPGradientDescentGaussianOptimizer(\n",
        "            l2_norm_clip=l2_norm_clip,\n",
        "            noise_multiplier=noise_multiplier,\n",
        "            num_microbatches=int(microbatches_perc * batch_size),\n",
        "            learning_rate=learning_rate)\n",
        "\n",
        "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.compat.v2.losses.Reduction.NONE)\n",
        "    else:\n",
        "        optimizer = GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.compat.v2.losses.Reduction.NONE)\n",
        "\n",
        "    classifier.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "    return classifier\n",
        "\n",
        "def shadow_model(feature_size, dpsgd, l2_norm_clip, noise_multiplier, microbatches_perc, learning_rate, kernel_regularization):\n",
        "    classifier = tf.keras.Sequential([\n",
        "        tf.keras.Input((feature_size), name='feature'),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "        tf.keras.layers.Dense(2, activation=tf.nn.softmax, kernel_regularizer=l1(kernel_regularization))\n",
        "    ])\n",
        "\n",
        "    if dpsgd:\n",
        "        optimizer = DPGradientDescentGaussianOptimizer(\n",
        "            l2_norm_clip=l2_norm_clip,\n",
        "            noise_multiplier=noise_multiplier,\n",
        "            num_microbatches=int(microbatches_perc * batch_size),\n",
        "            learning_rate=learning_rate)\n",
        "\n",
        "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.compat.v2.losses.Reduction.NONE)\n",
        "    else:\n",
        "        optimizer = GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.compat.v2.losses.Reduction.NONE)\n",
        "\n",
        "    classifier.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "    return classifier\n",
        "\n",
        "def personal_income_classification():\n",
        "    dataset = fetch_openml(name='adult', version=2, as_frame=True)\n",
        "    df = dataset.frame\n",
        "\n",
        "    df.columns = ['Age', 'Workclass', 'fnlwgt', 'Education', 'Education-num', 'Marital-status', 'Occupation',\n",
        "                  'Relationship', 'Race', 'Sex', 'Capital-gain', 'Capital-loss',\n",
        "                  'Hours-per-week', 'Native-country', 'Salary']\n",
        "\n",
        "    df['Age'] = df['Age'].astype(np.float32)\n",
        "    # Your existing data preprocessing code remains unchanged\n",
        "\n",
        "    df_train = df[['Age', 'Workclass', 'fnlwgt', 'Education', 'Education-num', 'Marital-status', 'Occupation',\n",
        "                   'Relationship', 'Race', 'Sex', 'Capital-gain', 'Capital-loss',\n",
        "                   'Hours-per-week', 'Native-country']]\n",
        "\n",
        "    df_test = df[['Salary']]\n",
        "\n",
        "    return df_train, df_test\n",
        "\n",
        "def main():\n",
        "    target_X_train, target_y_train, target_X_valid, target_y_valid = split_to_be_divisible(target_X,\n",
        "                                                                                           target_y,\n",
        "                                                                                           0.2,\n",
        "                                                                                           batch_size)\n",
        "\n",
        "    tm = target_model(feature_size, dpsgd, l2_norm_clip, noise_multiplier, microbatches_perc, learning_rate, kernel_regularization)\n",
        "    tm.fit(target_X_train.values,\n",
        "           target_y_train.values,\n",
        "           batch_size=batch_size,\n",
        "           epochs=epochs,\n",
        "           validation_data=[target_X_valid.values, target_y_valid.values],\n",
        "           verbose=1)\n",
        "\n",
        "    SHADOW_DATASET_SIZE = int(shadow_X.shape[0] / 2)\n",
        "    smb = ShadowModelBundle(\n",
        "        lambda: shadow_model(feature_size, dpsgd, l2_norm_clip, noise_multiplier, microbatches_perc, learning_rate, kernel_regularization),\n",
        "        shadow_dataset_size=SHADOW_DATASET_SIZE,\n",
        "        num_models=1,\n",
        "    )\n",
        "\n",
        "    attacker_X, attacker_y = smb.fit_transform(shadow_X.values, shadow_y.values,\n",
        "                                               fit_kwargs=dict(epochs=epochs,\n",
        "                                                              batch_size=batch_size,\n",
        "                                                              verbose=1))\n",
        "\n",
        "    clf = RandomForestClassifier(max_depth=2)\n",
        "    clf.fit(attacker_X, attacker_y)\n",
        "\n",
        "    ATTACK_TEST_DATASET_SIZE = unused_X.shape[0]\n",
        "    data_in = target_X_train[:ATTACK_TEST_DATASET_SIZE], target_y_train[:ATTACK_TEST_DATASET_SIZE]\n",
        "    data_out = unused_X[:ATTACK_TEST_DATASET_SIZE], unused_y[:ATTACK_TEST_DATASET_SIZE]\n",
        "\n",
        "    attack_test_data, real_membership_labels = prepare_attack_data(tm, data_in, data_out)\n",
        "\n",
        "    attack_guesses = clf.predict(attack_test_data)\n",
        "    attack_accuracy = np.mean(attack_guesses == real_membership_labels)\n",
        "    print('Attack accuracy: {}'.format(attack_accuracy))\n",
        "    acc = accuracy_score(real_membership_labels, attack_guesses)\n",
        "    print('Attack accuracy (using sklearn): {}'.format(acc))\n",
        "\n",
        "    prec = precision_score(real_membership_labels, attack_guesses)\n",
        "    print('Precision: {}'.format(prec))\n",
        "\n",
        "    recall = recall_score(real_membership_labels, attack_guesses)\n",
        "    print('Recall: {}'.format(recall))\n",
        "\n",
        "    fscore = f1_score(real_membership_labels, attack_guesses)\n",
        "    print('F1-Score: {}'.format(fscore))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    dpsgd = True\n",
        "    epochs = 10\n",
        "    batch_size = 32\n",
        "    microbatches_perc = 0.5\n",
        "    learning_rate = 0.01\n",
        "    kernel_regularization = 1.2\n",
        "    noise_multiplier = 0.8\n",
        "    l2_norm_clip = 0.8\n",
        "\n",
        "    print(\"Loading and splitting dataset\")\n",
        "\n",
        "    X, y = personal_income_classification()\n",
        "    target_X, target_y, shadow_X, shadow_y = split_to_be_divisible(X, y, 0.5, batch_size=80)\n",
        "\n",
        "    shadow_X, shadow_y, unused_X, unused_y = split_to_be_divisible(shadow_X, shadow_y, 0.3, batch_size)\n",
        "    feature_size = target_X.shape[1]\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "0fGd1L-VSyOM",
        "outputId": "bc652b92-7589-472a-c684-777485792973"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and splitting dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-c4ae309adfff>\u001b[0m in \u001b[0;36m<cell line: 153>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0mshadow_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshadow_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_to_be_divisible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshadow_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshadow_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0mfeature_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-c4ae309adfff>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mtm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpsgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_norm_clip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_multiplier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmicrobatches_perc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_regularization\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     tm.fit(target_X_train.values,\n\u001b[0m\u001b[1;32m    111\u001b[0m            \u001b[0mtarget_y_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m            \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n"
      ],
      "metadata": {
        "id": "AcpWTt0SUEyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n"
      ],
      "metadata": {
        "id": "fDMH7Ra7VQO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from tensorflow.keras.layers import Dense, Flatten, Input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.regularizers import l1\n",
        "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer\n",
        "from mia.estimators import ShadowModelBundle, prepare_attack_data\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(19122)\n",
        "\n",
        "GradientDescentOptimizer = tf.compat.v1.train.GradientDescentOptimizer\n",
        "\n",
        "def split_to_be_divisible(X, y, shadow_perc, batch_size):\n",
        "    assert y.shape[0] == X.shape[0]\n",
        "\n",
        "    total_row = X.shape[0]\n",
        "    num_shadow_row = int(total_row * shadow_perc) - int(total_row * shadow_perc) % batch_size\n",
        "    num_target_row = (total_row - num_shadow_row) - (total_row - num_shadow_row) % batch_size\n",
        "\n",
        "    random_row = np.random.permutation(total_row)\n",
        "    shadow_row = random_row[:num_shadow_row]\n",
        "    target_row = random_row[-num_target_row:]\n",
        "\n",
        "    target_X = X.iloc[target_row]\n",
        "    shadow_X = X.iloc[shadow_row]\n",
        "\n",
        "    target_y = y.iloc[target_row]\n",
        "    shadow_y = y.iloc[shadow_row]\n",
        "\n",
        "    return target_X, target_y, shadow_X, shadow_y\n",
        "\n",
        "def target_model(feature_size, dpsgd, l2_norm_clip, noise_multiplier, microbatches_perc, learning_rate, kernel_regularization):\n",
        "    model = Sequential([\n",
        "        Input(shape=(feature_size,), name='feature'),\n",
        "        Dense(128, activation=tf.nn.relu),\n",
        "        Dense(2, activation=tf.nn.softmax, kernel_regularizer=l1(kernel_regularization))\n",
        "    ])\n",
        "\n",
        "    if dpsgd:\n",
        "        optimizer = DPGradientDescentGaussianOptimizer(\n",
        "            l2_norm_clip=l2_norm_clip,\n",
        "            noise_multiplier=noise_multiplier,\n",
        "            num_microbatches=int(microbatches_perc * batch_size),\n",
        "            learning_rate=learning_rate)\n",
        "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.compat.v2.losses.Reduction.NONE)\n",
        "    else:\n",
        "        optimizer = GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.compat.v2.losses.Reduction.NONE)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def shadow_model(feature_size, dpsgd, l2_norm_clip, noise_multiplier, microbatches_perc, learning_rate, kernel_regularization):\n",
        "    model = Sequential([\n",
        "        Input(shape=(feature_size,), name='feature'),\n",
        "        Dense(128, activation=tf.nn.relu),\n",
        "        Dense(2, activation=tf.nn.softmax, kernel_regularizer=l1(kernel_regularization))\n",
        "    ])\n",
        "\n",
        "    if dpsgd:\n",
        "        optimizer = DPGradientDescentGaussianOptimizer(\n",
        "            l2_norm_clip=l2_norm_clip,\n",
        "            noise_multiplier=noise_multiplier,\n",
        "            num_microbatches=int(microbatches_perc * batch_size),\n",
        "            learning_rate=learning_rate)\n",
        "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.compat.v2.losses.Reduction.NONE)\n",
        "    else:\n",
        "        optimizer = GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.compat.v2.losses.Reduction.NONE)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def personal_income_classification():\n",
        "    dataset = fetch_openml(name='adult', version=2, as_frame=True)\n",
        "    df = dataset.frame\n",
        "\n",
        "    df.columns = ['Age', 'Workclass', 'fnlwgt', 'Education', 'Education-num', 'Marital-status', 'Occupation',\n",
        "                  'Relationship', 'Race', 'Sex', 'Capital-gain', 'Capital-loss',\n",
        "                  'Hours-per-week', 'Native-country', 'Salary']\n",
        "\n",
        "    df['Age'] = df['Age'].astype(np.float32)\n",
        "    # Your existing data preprocessing code remains unchanged\n",
        "\n",
        "    df_train = df[['Age', 'Workclass', 'fnlwgt', 'Education', 'Education-num', 'Marital-status', 'Occupation',\n",
        "                   'Relationship', 'Race', 'Sex', 'Capital-gain', 'Capital-loss',\n",
        "                   'Hours-per-week', 'Native-country']]\n",
        "\n",
        "    df_test = df[['Salary']]\n",
        "\n",
        "    return df_train, df_test\n",
        "\n",
        "def main():\n",
        "    target_X_train, target_y_train, target_X_valid, target_y_valid = split_to_be_divisible(target_X,\n",
        "                                                                                           target_y,\n",
        "                                                                                           0.2,\n",
        "                                                                                           batch_size)\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "    target_y_train_encoded = label_encoder.fit_transform(target_y_train)\n",
        "    target_y_valid_encoded = label_encoder.transform(target_y_valid)\n",
        "\n",
        "    # Convert numerical labels to one-hot encoding\n",
        "    target_y_train_one_hot = to_categorical(target_y_train_encoded)\n",
        "    target_y_valid_one_hot = to_categorical(target_y_valid_encoded)\n",
        "\n",
        "    tm = target_model(feature_size, dpsgd, l2_norm_clip, noise_multiplier, microbatches_perc, learning_rate, kernel_regularization)\n",
        "    tm.fit(target_X_train.values,\n",
        "           target_y_train_one_hot,\n",
        "           batch_size=batch_size,\n",
        "           epochs=epochs,\n",
        "           validation_data=(target_X_valid.values, target_y_valid_one_hot),\n",
        "           verbose=1)\n",
        "\n",
        "    SHADOW_DATASET_SIZE = int(shadow_X.shape[0] / 2)\n",
        "    smb = ShadowModelBundle(\n",
        "        lambda: shadow_model(feature_size, dpsgd, l2_norm_clip, noise_multiplier, microbatches_perc, learning_rate, kernel_regularization),\n",
        "        shadow_dataset_size=SHADOW_DATASET_SIZE,\n",
        "        num_models=1,\n",
        "    )\n",
        "\n",
        "    attacker_X, attacker_y = smb.fit_transform(shadow_X.values, shadow_y.values,\n",
        "                                               fit_kwargs=dict(epochs=epochs,\n",
        "                                                              batch_size=batch_size,\n",
        "                                                              verbose=1))\n",
        "\n",
        "    clf = RandomForestClassifier(max_depth=2)\n",
        "    clf.fit(attacker_X, attacker_y)\n",
        "\n",
        "    ATTACK_TEST_DATASET_SIZE = unused_X.shape[0]\n",
        "    data_in = target_X_train[:ATTACK_TEST_DATASET_SIZE], target_y_train[:ATTACK_TEST_DATASET_SIZE]\n",
        "    data_out = unused_X[:ATTACK_TEST_DATASET_SIZE], unused_y[:ATTACK_TEST_DATASET_SIZE]\n",
        "\n",
        "    attack_test_data, real_membership_labels = prepare_attack_data(tm, data_in, data_out)\n",
        "\n",
        "    attack_guesses = clf.predict(attack_test_data)\n",
        "    attack_accuracy = np.mean(attack_guesses == real_membership_labels)\n",
        "    print('Attack accuracy: {}'.format(attack_accuracy))\n",
        "    acc = accuracy_score(real_membership_labels, attack_guesses)\n",
        "    print('Attack accuracy (using sklearn): {}'.format(acc))\n",
        "\n",
        "    prec = precision_score(real_membership_labels, attack_guesses)\n",
        "    print('Precision: {}'.format(prec))\n",
        "\n",
        "    recall = recall_score(real_membership_labels, attack_guesses)\n",
        "    print('Recall: {}'.format(recall))\n",
        "\n",
        "    fscore = f1_score(real_membership_labels, attack_guesses)\n",
        "    print('F1-Score: {}'.format(fscore))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    dpsgd = True\n",
        "    epochs = 10\n",
        "    batch_size = 32\n",
        "    microbatches_perc = 0.5\n",
        "    learning_rate = 0.01\n",
        "    kernel_regularization = 1.2\n",
        "    noise_multiplier = 0.8\n",
        "    l2_norm_clip = 0.8\n",
        "\n",
        "    print(\"Loading and splitting dataset\")\n",
        "\n",
        "    X, y = personal_income_classification()\n",
        "    target_X, target_y, shadow_X, shadow_y = split_to_be_divisible(X, y, 0.5, batch_size=80)\n",
        "\n",
        "    shadow_X, shadow_y, unused_X, unused_y = split_to_be_divisible(shadow_X, shadow_y, 0.3, batch_size)\n",
        "    feature_size = target_X.shape[1]\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "GhiUpgs5Tghz",
        "outputId": "ff6aea84-64f3-49ca-a5d7-6ce5f4a0ed7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and splitting dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-122a8a243fce>\u001b[0m in \u001b[0;36m<cell line: 157>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0mshadow_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshadow_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_to_be_divisible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshadow_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshadow_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0mfeature_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-122a8a243fce>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0mtm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpsgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_norm_clip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_multiplier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmicrobatches_perc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_regularization\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     tm.fit(target_X_train.values,\n\u001b[0m\u001b[1;32m    115\u001b[0m            \u001b[0mtarget_y_train_one_hot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m            \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Dense, Input, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer\n",
        "from mia.estimators import ShadowModelBundle, prepare_attack_data\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(19122)\n",
        "\n",
        "GradientDescentOptimizer = tf.compat.v1.train.GradientDescentOptimizer\n",
        "\n",
        "def split_to_be_divisible(X, y, shadow_perc, batch_size):\n",
        "    assert y.shape[0] == X.shape[0]\n",
        "\n",
        "    total_row = X.shape[0]\n",
        "    num_shadow_row = int(total_row * shadow_perc) - int(total_row * shadow_perc) % batch_size\n",
        "    num_target_row = (total_row - num_shadow_row) - (total_row - num_shadow_row) % batch_size\n",
        "\n",
        "    random_row = np.random.permutation(total_row)\n",
        "    shadow_row = random_row[:num_shadow_row]\n",
        "    target_row = random_row[-num_target_row:]\n",
        "\n",
        "    target_X = X.iloc[target_row]\n",
        "    shadow_X = X.iloc[shadow_row]\n",
        "\n",
        "    target_y = y.iloc[target_row]\n",
        "    shadow_y = y.iloc[shadow_row]\n",
        "\n",
        "    return target_X, target_y, shadow_X, shadow_y\n",
        "\n",
        "def target_model(feature_size, dpsgd, l2_norm_clip, noise_multiplier, microbatches_perc, learning_rate, kernel_regularization):\n",
        "    model = Sequential([\n",
        "        Input(shape=(feature_size,), name='feature'),\n",
        "        Dense(128, activation=tf.nn.relu),\n",
        "        Dense(3, activation=tf.nn.softmax, kernel_regularizer=l1(kernel_regularization))  # 3 classes for Iris dataset\n",
        "    ])\n",
        "\n",
        "    if dpsgd:\n",
        "        optimizer = DPGradientDescentGaussianOptimizer(\n",
        "            l2_norm_clip=l2_norm_clip,\n",
        "            noise_multiplier=noise_multiplier,\n",
        "            num_microbatches=int(microbatches_perc * batch_size),\n",
        "            learning_rate=learning_rate)\n",
        "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.compat.v2.losses.Reduction.NONE)\n",
        "    else:\n",
        "        optimizer = GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.compat.v2.losses.Reduction.NONE)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def shadow_model(feature_size, dpsgd, l2_norm_clip, noise_multiplier, microbatches_perc, learning_rate, kernel_regularization):\n",
        "    model = Sequential([\n",
        "        Input(shape=(feature_size,), name='feature'),\n",
        "        Dense(128, activation=tf.nn.relu),\n",
        "        Dense(3, activation=tf.nn.softmax, kernel_regularizer=l1(kernel_regularization))  # 3 classes for Iris dataset\n",
        "    ])\n",
        "\n",
        "    if dpsgd:\n",
        "        optimizer = DPGradientDescentGaussianOptimizer(\n",
        "            l2_norm_clip=l2_norm_clip,\n",
        "            noise_multiplier=noise_multiplier,\n",
        "            num_microbatches=int(microbatches_perc * batch_size),\n",
        "            learning_rate=learning_rate)\n",
        "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.compat.v2.losses.Reduction.NONE)\n",
        "    else:\n",
        "        optimizer = GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.compat.v2.losses.Reduction.NONE)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def iris_classification():\n",
        "    iris = load_iris()\n",
        "    X = iris.data\n",
        "    y = iris.target\n",
        "\n",
        "    # Convert labels to one-hot encoding\n",
        "    y_one_hot = to_categorical(y)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "def main():\n",
        "    X_train, y_train, X_test, y_test = iris_classification()\n",
        "\n",
        "    y_train_int = np.argmax(y_train, axis=1)\n",
        "    y_test_int = np.argmax(y_test, axis=1)\n",
        "\n",
        "    tm = target_model(X_train.shape[1], dpsgd, l2_norm_clip, noise_multiplier, microbatches_perc, learning_rate, kernel_regularization)\n",
        "    tm.fit(X_train,\n",
        "           y_train_int,\n",
        "           batch_size=batch_size,\n",
        "           epochs=epochs,\n",
        "           validation_data=(X_test, y_test_int),\n",
        "           verbose=1)\n",
        "\n",
        "    SHADOW_DATASET_SIZE = int(X_train.shape[0] / 2)\n",
        "    smb = ShadowModelBundle(\n",
        "        lambda: shadow_model(X_train.shape[1], dpsgd, l2_norm_clip, noise_multiplier, microbatches_perc, learning_rate, kernel_regularization),\n",
        "        shadow_dataset_size=SHADOW_DATASET_SIZE,\n",
        "        num_models=1,\n",
        "    )\n",
        "\n",
        "    attacker_X, attacker_y = smb.fit_transform(X_train, np.argmax(y_train, axis=1),\n",
        "                                               fit_kwargs=dict(epochs=epochs,\n",
        "                                                              batch_size=batch_size,\n",
        "                                                              verbose=1))\n",
        "\n",
        "    clf = RandomForestClassifier(max_depth=2)\n",
        "    clf.fit(attacker_X, attacker_y)\n",
        "\n",
        "    ATTACK_TEST_DATASET_SIZE = X_test.shape[0]\n",
        "    data_in = X_train[:ATTACK_TEST_DATASET_SIZE], np.argmax(y_train[:ATTACK_TEST_DATASET_SIZE], axis=1)\n",
        "    data_out = X_test[:ATTACK_TEST_DATASET_SIZE], np.argmax(y_test[:ATTACK_TEST_DATASET_SIZE], axis=1)\n",
        "\n",
        "    attack_test_data, real_membership_labels = prepare_attack_data(tm, data_in, data_out)\n",
        "\n",
        "    attack_guesses = clf.predict(attack_test_data)\n",
        "    attack_accuracy = accuracy_score(real_membership_labels, attack_guesses)\n",
        "    print('Attack accuracy: {}'.format(attack_accuracy))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    dpsgd = True\n",
        "    epochs = 10\n",
        "    batch_size = 32\n",
        "    microbatches_perc = 0.5\n",
        "    learning_rate = 0.01\n",
        "    kernel_regularization = 1.2\n",
        "    noise_multiplier = 0.8\n",
        "    l2_norm_clip = 0.8\n",
        "\n",
        "    print(\"Training the models on Iris dataset\")\n",
        "\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "qVs8W8UAXlPc",
        "outputId": "c81e6dc8-dafd-40fa-b027-133b0deb8fe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the models on Iris dataset\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 328, in __call__\n        total_total_loss_mean_value = tf.add_n(total_loss_mean_values)\n\n    ValueError: Shapes must be equal rank, but are 1 and 0\n    \tFrom merging shape 0 with other shapes. for '{{node AddN}} = AddN[N=2, T=DT_FLOAT](sparse_categorical_crossentropy/weighted_loss/Mul, dense_19/kernel/Regularizer/mul)' with input shapes: [?], [].\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-6fd39ef785f0>\u001b[0m in \u001b[0;36m<cell line: 132>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training the models on Iris dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-6fd39ef785f0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mtm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpsgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_norm_clip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_multiplier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmicrobatches_perc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_regularization\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     tm.fit(X_train,\n\u001b[0m\u001b[1;32m    101\u001b[0m            \u001b[0my_train_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m            \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 328, in __call__\n        total_total_loss_mean_value = tf.add_n(total_loss_mean_values)\n\n    ValueError: Shapes must be equal rank, but are 1 and 0\n    \tFrom merging shape 0 with other shapes. for '{{node AddN}} = AddN[N=2, T=DT_FLOAT](sparse_categorical_crossentropy/weighted_loss/Mul, dense_19/kernel/Regularizer/mul)' with input shapes: [?], [].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# depent on tensorflow 1.14\n",
        "import tensorflow as tf\n",
        "from mia.estimators import ShadowModelBundle, prepare_attack_data\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.utils import resample\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.regularizers import l1\n",
        "# privacy package\n",
        "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer\n",
        "\n",
        "# set random seed\n",
        "np.random.seed(19122)\n",
        "\n",
        "GradientDescentOptimizer = tf.compat.v1.train.GradientDescentOptimizer\n",
        "\n",
        "\n",
        "class DataGenerator(object):\n",
        "    \"\"\"\n",
        "    Load and preprocess data: filter NA, binarize phenotype, balance sample.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, genotype_file, phenotype_file, shuffle=True):\n",
        "        super(DataGenerator, self).__init__()\n",
        "        self.genotype_file = genotype_file\n",
        "        self.phenotype_file = phenotype_file\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        # load data\n",
        "        self._load_data()\n",
        "        # preprocess\n",
        "        self._filter_na_phenotype()\n",
        "        self._binarize_phenotype()\n",
        "        self._balance_sample()\n",
        "\n",
        "    def _load_data(self):\n",
        "        self.genotype = pd.read_csv(genotype_file, sep='\\t', index_col=0)\n",
        "        # print('genotype_file shape:', genotype.shape)\n",
        "\n",
        "        self.multi_pheno = pd.read_csv(phenotype_file, sep=',', index_col=0)\n",
        "        self.phenotype = self.multi_pheno.iloc[:, 2]\n",
        "        # print('phenotype shape:', phenotype.shape)\n",
        "\n",
        "    def _filter_na_phenotype(self):\n",
        "        missing_mask = self.phenotype.isna()\n",
        "\n",
        "        self.genotype = self.genotype[~missing_mask]\n",
        "        self.phenotype = self.phenotype[~missing_mask]\n",
        "\n",
        "    def _binarize_phenotype(self):\n",
        "        self.phenotype[self.phenotype > 0] = 1\n",
        "        self.phenotype[self.phenotype < 0] = 0\n",
        "\n",
        "    def _balance_sample(self):\n",
        "        # find majority and minority\n",
        "        num_zeros = self.phenotype.value_counts()[0]\n",
        "        num_ones = self.phenotype.value_counts()[1]\n",
        "        if num_ones > num_zeros:\n",
        "            majority = 1\n",
        "            minority = 0\n",
        "        else:\n",
        "            majority = 0\n",
        "            minority = 1\n",
        "\n",
        "        # downsampling majority\n",
        "        majority_index = self.phenotype == majority\n",
        "        minority_index = self.phenotype == minority\n",
        "        majority_downsampled = resample(\n",
        "            self.genotype[majority_index],\n",
        "            replace=True,\n",
        "            n_samples=self.phenotype.value_counts()[minority],\n",
        "        )\n",
        "\n",
        "        self.genotype = pd.concat(\n",
        "            [majority_downsampled, self.genotype[minority_index]], axis=0)\n",
        "        if self.shuffle:\n",
        "            self.genotype = self.genotype.sample(frac=1)\n",
        "        self.phenotype = self.phenotype[self.genotype.index]\n",
        "\n",
        "\n",
        "def split_to_be_divisible(X, y, shadow_perc, batch_size):\n",
        "    \"\"\"\n",
        "    Split a dataframe into target dataset and shadow dataset, and make them divisible by batch size.\n",
        "\n",
        "    :param X: genotype data\n",
        "    :param y: phenotype data\n",
        "    :param shadow_perc: specified percent for shadow dataset, target_perc = 1 - shadow_perc\n",
        "    :param batch_size: batch_size for training process\n",
        "\n",
        "    :return: target datasets, shadow datasets\n",
        "    \"\"\"\n",
        "\n",
        "    # stop and output error, if X and y have different number of individuals.\n",
        "    assert y.shape[0] == X.shape[0]\n",
        "\n",
        "    # calculate sample size of target and shadow\n",
        "    total_row = X.shape[0]\n",
        "    num_shadow_row = int(total_row * shadow_perc) - int(total_row * shadow_perc) % batch_size\n",
        "    num_target_row = (total_row - num_shadow_row) - (total_row - num_shadow_row) % batch_size\n",
        "\n",
        "    # split train and valid\n",
        "    random_row = np.random.permutation(total_row)\n",
        "    shadow_row = random_row[:num_shadow_row]\n",
        "    target_row = random_row[-num_target_row:]\n",
        "\n",
        "    target_X = X.iloc[target_row]\n",
        "    shadow_X = X.iloc[shadow_row]\n",
        "\n",
        "    target_y = y.iloc[target_row]\n",
        "    shadow_y = y.iloc[shadow_row]\n",
        "\n",
        "    return target_X, target_y, shadow_X, shadow_y\n",
        "\n",
        "\n",
        "def target_model():\n",
        "    \"\"\"The architecture of the target model.\n",
        "    The attack is white-box, hence the attacker is assumed to know this architecture too.\n",
        "\n",
        "    :return: target model\n",
        "    \"\"\"\n",
        "\n",
        "    classifier = Sequential()\n",
        "    classifier.add(\n",
        "        Dense(1,\n",
        "              input_dim=feature_size,\n",
        "              kernel_regularizer=l1(kernel_regularization),\n",
        "              activation='sigmoid'))\n",
        "\n",
        "    if dpsgd:\n",
        "        optimizer = DPGradientDescentGaussianOptimizer(\n",
        "            l2_norm_clip=l2_norm_clip,\n",
        "            noise_multiplier=noise_multiplier,\n",
        "            num_microbatches=int(microbatches_perc * batch_size),\n",
        "            learning_rate=learning_rate)\n",
        "        # Compute vector of per-example loss rather than its mean over a minibatch.\n",
        "        loss = tf.keras.losses.BinaryCrossentropy(\n",
        "            from_logits=True, reduction=tf.compat.v2.losses.Reduction.NONE)\n",
        "    else:\n",
        "        optimizer = GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "        loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "    # Compile model with Keras\n",
        "    classifier.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "    return classifier\n",
        "\n",
        "\n",
        "def shadow_model():\n",
        "    \"\"\"The architecture of the shadow model is same as target model, because the attack is white-box,\n",
        "    hence the attacker is assumed to know this architecture too.\n",
        "\n",
        "    :return: shadow model\n",
        "    \"\"\"\n",
        "\n",
        "    classifier = Sequential()\n",
        "    classifier.add(\n",
        "        Dense(1,\n",
        "              input_dim=feature_size,\n",
        "              kernel_regularizer=l1(kernel_regularization),\n",
        "              activation='sigmoid'))\n",
        "\n",
        "    if dpsgd:\n",
        "        optimizer = DPGradientDescentGaussianOptimizer(\n",
        "            l2_norm_clip=l2_norm_clip,\n",
        "            noise_multiplier=noise_multiplier,\n",
        "            num_microbatches=int(microbatches_perc * batch_size),\n",
        "            learning_rate=learning_rate)\n",
        "        # Compute vector of per-example loss rather than its mean over a minibatch.\n",
        "        loss = tf.keras.losses.BinaryCrossentropy(\n",
        "            from_logits=True, reduction=tf.compat.v2.losses.Reduction.NONE)\n",
        "    else:\n",
        "        optimizer = GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "        loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "    # Compile model with Keras\n",
        "    classifier.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "    return classifier\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"Training the target model...\")\n",
        "    # split target dataset to train and valid, and make them evenly divisible by batch size\n",
        "    target_X_train, target_y_train, target_X_valid, target_y_valid = split_to_be_divisible(target_X,\n",
        "                                                                                           target_y,\n",
        "                                                                                           0.2,\n",
        "                                                                                           batch_size)\n",
        "\n",
        "    tm = target_model()\n",
        "    tm.fit(target_X_train.values,\n",
        "           target_y_train.values,\n",
        "           batch_size=batch_size,\n",
        "           epochs=epochs,\n",
        "           validation_data=[target_X_valid.values, target_y_valid.values],\n",
        "           verbose=1)\n",
        "\n",
        "    print(\"Training the shadow models.\")\n",
        "    # train only one shadow model\n",
        "    SHADOW_DATASET_SIZE = int(shadow_X.shape[0] / 2)\n",
        "    smb = ShadowModelBundle(\n",
        "        shadow_model,\n",
        "        shadow_dataset_size=SHADOW_DATASET_SIZE,\n",
        "        num_models=1,\n",
        "    )\n",
        "    # Training the shadow models with same parameter of target model, and generate attack data...\n",
        "    attacker_X, attacker_y = smb.fit_transform(shadow_X.values, shadow_y.values,\n",
        "                                               fit_kwargs=dict(epochs=epochs,\n",
        "                                                               batch_size=batch_size,\n",
        "                                                               verbose=1),\n",
        "                                               )\n",
        "\n",
        "    print(\"Training attack model...\")\n",
        "    clf = RandomForestClassifier(max_depth=2)\n",
        "    clf.fit(attacker_X, attacker_y)\n",
        "\n",
        "    # Test the success of the attack.\n",
        "    ATTACK_TEST_DATASET_SIZE = unused_X.shape[0]\n",
        "    # Prepare examples that were in the training, and out of the training.\n",
        "    data_in = target_X_train[:ATTACK_TEST_DATASET_SIZE], target_y_train[:ATTACK_TEST_DATASET_SIZE]\n",
        "    data_out = unused_X[:ATTACK_TEST_DATASET_SIZE], unused_y[:ATTACK_TEST_DATASET_SIZE]\n",
        "\n",
        "    # Compile them into the expected format for the AttackModelBundle.\n",
        "    attack_test_data, real_membership_labels = prepare_attack_data(tm, data_in, data_out)\n",
        "\n",
        "    # Compute the attack accuracy.\n",
        "    attack_guesses = clf.predict(attack_test_data)\n",
        "    attack_accuracy = np.mean(attack_guesses == real_membership_labels)\n",
        "\n",
        "    print('attack accuracy: {}'.format(attack_accuracy))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # genotype\n",
        "    genotype_file = '../data/genotype_full.txt'\n",
        "    # phenotype\n",
        "    phenotype_file = '../data/phenotype.csv'\n",
        "\n",
        "    # parameters\n",
        "    dpsgd = True\n",
        "\n",
        "    # target model hyper-parameters same as Lasso-dp\n",
        "    epochs = 50\n",
        "    batch_size = 16\n",
        "    microbatches_perc = 1.0\n",
        "    learning_rate = 0.01\n",
        "    kernel_regularization = 0.001352\n",
        "    noise_multiplier = 0.8\n",
        "    l2_norm_clip = 1.0\n",
        "\n",
        "    print(\"Loading and split dataset\")\n",
        "    yeast = DataGenerator(genotype_file, phenotype_file)\n",
        "    target_X, target_y, shadow_X, shadow_y = split_to_be_divisible(yeast.genotype,\n",
        "                                                                   yeast.phenotype,\n",
        "                                                                   0.5,\n",
        "                                                                   batch_size=80)\n",
        "    shadow_X, shadow_y, unused_X, unused_y = split_to_be_divisible(shadow_X,\n",
        "                                                                   shadow_y,\n",
        "                                                                   0.2,\n",
        "                                                                   batch_size)\n",
        "\n",
        "    feature_size = target_X.shape[1]\n",
        "\n",
        "    # # define the grid search parameters\n",
        "    # if dpsgd:\n",
        "    #     param_grid = {\n",
        "    #         'epochs': [50, 100],\n",
        "    #         'batch_size': [8, 16],\n",
        "    #         'microbatches_perc': [0.5, 1],\n",
        "    #         'learning_rate': [0.01, 0.001],\n",
        "    #         'kernel_regularization': [0, 0.001352],\n",
        "    #         'noise_multiplier': [0.4, 0.6, 0.8, 1.0, 1.2],\n",
        "    #         'l2_norm_clip': [0.6, 1.0, 1.4, 1.8],\n",
        "    #         'verbose': [0]\n",
        "    #     }\n",
        "    # else:\n",
        "    #     # define the grid search parameters\n",
        "    #     param_grid = {\n",
        "    #         'epochs': [50, 100],\n",
        "    #         'batch_size': [8, 16],\n",
        "    #         'learning_rate': [0.01, 0.001],\n",
        "    #         'kernel_regularization': [0, 0.001352],\n",
        "    #         'verbose': [0]\n",
        "    #     }\n",
        "\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "HifXStZpY-hU",
        "outputId": "f02d3b01-cb46-4789-c406-9064f7bffad0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and split dataset\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../data/genotype_full.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-143b5db30fcb>\u001b[0m in \u001b[0;36m<cell line: 236>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading and split dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m     \u001b[0myeast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenotype_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphenotype_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m     target_X, target_y, shadow_X, shadow_y = split_to_be_divisible(yeast.genotype,\n\u001b[1;32m    257\u001b[0m                                                                    \u001b[0myeast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphenotype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-143b5db30fcb>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, genotype_file, phenotype_file, shuffle)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;31m# preprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filter_na_phenotype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-143b5db30fcb>\u001b[0m in \u001b[0;36m_load_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenotype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenotype_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;31m# print('genotype_file shape:', genotype.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/genotype_full.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "path = \"./\"\n",
        "\n",
        "# depent on tensorflow 1.14\n",
        "import tensorflow as tf\n",
        "from mia.estimators import ShadowModelBundle, prepare_attack_data\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.utils import resample\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.regularizers import l1\n",
        "# privacy package\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer\n",
        "\n",
        "\n",
        "# set random seed\n",
        "import random\n",
        "random.seed(19122)\n",
        "\n",
        "# set random seed\n",
        "np.random.seed(19122)\n",
        "# tf.set_random_seed(19122)\n",
        "\n",
        "GradientDescentOptimizer = tf.compat.v1.train.GradientDescentOptimizer\n",
        "\n",
        "def split_to_be_divisible(X, y, shadow_perc, batch_size):\n",
        "    \"\"\"\n",
        "    Split a dataframe into target dataset and shadow dataset, and make them divisible by batch size.\n",
        "\n",
        "    :param X: genotype data\n",
        "    :param y: phenotype data\n",
        "    :param shadow_perc: specified percent for shadow dataset, target_perc = 1 - shadow_perc\n",
        "    :param batch_size: batch_size for training process\n",
        "\n",
        "    :return: target datasets, shadow datasets\n",
        "    \"\"\"\n",
        "\n",
        "    # stop and output error, if X and y have different number of individuals.\n",
        "    print(y.shape[0] == X.shape[0])\n",
        "    assert y.shape[0] == X.shape[0]\n",
        "\n",
        "    # calculate sample size of target and shadow\n",
        "    total_row = X.shape[0]\n",
        "    num_shadow_row = int(total_row * shadow_perc) - int(total_row * shadow_perc) % batch_size\n",
        "    num_target_row = (total_row - num_shadow_row) - (total_row - num_shadow_row) % batch_size\n",
        "\n",
        "    # split train and valid\n",
        "    random_row = np.random.permutation(total_row)\n",
        "    shadow_row = random_row[:num_shadow_row]\n",
        "    target_row = random_row[-num_target_row:]\n",
        "\n",
        "    target_X = X.iloc[target_row]\n",
        "    shadow_X = X.iloc[shadow_row]\n",
        "\n",
        "    target_y = y.iloc[target_row]\n",
        "    shadow_y = y.iloc[shadow_row]\n",
        "\n",
        "    return target_X, target_y, shadow_X, shadow_y\n",
        "\n",
        "\n",
        "def target_model():\n",
        "    \"\"\"The architecture of the target model.\n",
        "    The attack is white-box, hence the attacker is assumed to know this architecture too.\n",
        "\n",
        "    :return: target model\n",
        "    \"\"\"\n",
        "    classifier = tf.keras.Sequential([\n",
        "        tf.keras.Input((feature_size), name='feature'),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "        tf.keras.layers.Dense(2, activation=tf.nn.softmax, kernel_regularizer=l1(kernel_regularization))\n",
        "    ])\n",
        "\n",
        "    if dpsgd:\n",
        "        optimizer = DPGradientDescentGaussianOptimizer(\n",
        "            l2_norm_clip=l2_norm_clip,\n",
        "            noise_multiplier=noise_multiplier,\n",
        "            num_microbatches=int(microbatches_perc * batch_size),\n",
        "            learning_rate=learning_rate)\n",
        "\n",
        "        # Compute vector of per-example loss rather than its mean over a minibatch.\n",
        "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.compat.v2.losses.Reduction.NONE)\n",
        "    else:\n",
        "        optimizer = GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.compat.v2.losses.Reduction.NONE)\n",
        "\n",
        "    # Compile model with Keras\n",
        "    classifier.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "    return classifier\n",
        "\n",
        "\n",
        "def shadow_model():\n",
        "    \"\"\"The architecture of the shadow model is same as target model, because the attack is white-box,\n",
        "    hence the attacker is assumed to know this architecture too.\n",
        "\n",
        "    :return: shadow model\n",
        "    \"\"\"\n",
        "\n",
        "    classifier = Sequential()\n",
        "    classifier = tf.keras.Sequential([\n",
        "        tf.keras.Input((feature_size), name='feature'),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "        tf.keras.layers.Dense(2, activation=tf.nn.softmax,  kernel_regularizer=l1(kernel_regularization))\n",
        "    ])\n",
        "\n",
        "    if dpsgd:\n",
        "        optimizer = DPGradientDescentGaussianOptimizer(\n",
        "            l2_norm_clip=l2_norm_clip,\n",
        "            noise_multiplier=noise_multiplier,\n",
        "            num_microbatches=int(microbatches_perc * batch_size),\n",
        "            learning_rate=learning_rate)\n",
        "        # Compute vector of per-example loss rather than its mean over a minibatch.\n",
        "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.compat.v2.losses.Reduction.NONE)\n",
        "\n",
        "    else:\n",
        "        optimizer = GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.compat.v2.losses.Reduction.NONE)\n",
        "\n",
        "    # Compile model with Keras\n",
        "    classifier.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "    return classifier\n",
        "\n",
        "\n",
        "def  personal_income_classification():\n",
        "    \"\"\"\n",
        "        Load a dataset and encode categorical variables.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    df = pd.read_csv(path + \"adult.data\", sep=\",\", engine=\"python\")\n",
        "\n",
        "    print(df.shape)\n",
        "    print(df.columns)\n",
        "\n",
        "    # Renaming the columns\n",
        "    df.columns = ['Age', 'Workclass', 'fnlwgt', 'Education', 'Education-num', 'Marital-status', 'Occupation',\n",
        "                  'Relationship', 'Race', 'Sex', 'Capital-gain', 'Capital-loss',\n",
        "                  'Hours-per-week', 'Native-country', 'Salary']\n",
        "\n",
        "    df['Age'] = df['Age'].astype(np.float32)\n",
        "\n",
        "    df['fnlwgt'] = df['fnlwgt'].astype(np.float32)\n",
        "    df['Education-num'] = df['Education-num'].astype(np.float32)\n",
        "    df['Capital-gain'] = df['Capital-gain'].astype(np.float32)\n",
        "    df['Capital-loss'] = df['Capital-loss'].astype(np.float32)\n",
        "    df['Hours-per-week'] = df['Hours-per-week'].astype(np.float32)\n",
        "\n",
        "    df['Workclass'] = df['Workclass'].astype('category').cat.codes.astype(np.float32)\n",
        "    df['Education'] = df['Education'].astype('category').cat.codes.astype(np.float32)\n",
        "    df['Marital-status'] = df['Marital-status'].astype('category').cat.codes.astype(np.float32)\n",
        "    df['Occupation'] = df['Occupation'].astype('category').cat.codes.astype(np.float32)\n",
        "    df['Relationship'] = df['Relationship'].astype('category').cat.codes.astype(np.float32)\n",
        "    df['Race'] = df['Race'].astype('category').cat.codes.astype(np.float32)\n",
        "    df['Sex'] = df['Sex'].astype('category').cat.codes.astype(np.float32)\n",
        "    df['Native-country'] = df['Native-country'].astype('category').cat.codes.astype(np.float32)\n",
        "    df['Salary'] = df['Salary'].astype('category').cat.codes.astype(np.float32)\n",
        "\n",
        "    df_train = df[\n",
        "        ['Age', 'Workclass', 'fnlwgt', 'Education', 'Education-num', 'Marital-status', 'Occupation', 'Relationship',\n",
        "         'Race', 'Sex', 'Capital-gain', 'Capital-loss',\n",
        "         'Hours-per-week', 'Native-country']]\n",
        "\n",
        "    df_test = df[['Salary']]\n",
        "\n",
        "    print(df.head())\n",
        "    return df_train, df_test\n",
        "\n",
        "def main():\n",
        "    print(\"Training the target model...\")\n",
        "    # split target dataset to train and valid, and make them evenly divisible by batch size\n",
        "\n",
        "    target_X_train, target_y_train, target_X_valid, target_y_valid = split_to_be_divisible(target_X,\n",
        "                                                                                           target_y,\n",
        "                                                                                           0.2,\n",
        "                                                                                           batch_size)\n",
        "    y_train_int = np.argmax(y_train, axis=1)\n",
        "    y_test_int = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Check data shapes before training\n",
        "    print(\"X_train shape:\", X_train.shape)\n",
        "    print(\"y_train_int shape:\", y_train_int.shape)\n",
        "\n",
        "    tm = target_model()\n",
        "    tm.fit(target_X_train,\n",
        "           target_y_train,\n",
        "           batch_size=batch_size,\n",
        "           epochs=epochs,\n",
        "           validation_data=[target_X_valid.values, target_y_valid.values],\n",
        "           verbose=1)\n",
        "\n",
        "    print(\"Training the shadow models.\")\n",
        "    # train only one shadow model\n",
        "    SHADOW_DATASET_SIZE = int(shadow_X.shape[0] / 2)\n",
        "    smb = ShadowModelBundle(\n",
        "        shadow_model,\n",
        "        shadow_dataset_size=SHADOW_DATASET_SIZE,\n",
        "        num_models=1,\n",
        "    )\n",
        "    # Training the shadow models with same parameter of target model, and generate attack data...\n",
        "    attacker_X, attacker_y = smb.fit_transform(shadow_X.values, shadow_y.values,\n",
        "                                               fit_kwargs=dict(epochs=epochs,\n",
        "                                                               batch_size=batch_size,\n",
        "                                                               verbose=1),\n",
        "                                               )\n",
        "\n",
        "    print(\"Training attack model...\")\n",
        "    clf = RandomForestClassifier(max_depth=2)\n",
        "    clf.fit(attacker_X, attacker_y)\n",
        "\n",
        "    # Test the success of the attack.\n",
        "    ATTACK_TEST_DATASET_SIZE = unused_X.shape[0]\n",
        "    # Prepare examples that were in the training, and out of the training.\n",
        "    data_in = target_X_train[:ATTACK_TEST_DATASET_SIZE], target_y_train[:ATTACK_TEST_DATASET_SIZE]\n",
        "    data_out = unused_X[:ATTACK_TEST_DATASET_SIZE], unused_y[:ATTACK_TEST_DATASET_SIZE]\n",
        "\n",
        "    # Compile them into the expected format for the AttackModelBundle.\n",
        "    attack_test_data, real_membership_labels = prepare_attack_data(tm, data_in, data_out)\n",
        "\n",
        "    # Compute the attack accuracy.\n",
        "    attack_guesses = clf.predict(attack_test_data)\n",
        "    attack_accuracy = np.mean(attack_guesses == real_membership_labels)\n",
        "    print('attack accuracy: {}'.format(attack_accuracy))\n",
        "    acc = accuracy_score(real_membership_labels, attack_guesses)\n",
        "    print('attack acc: {}'.format(acc))\n",
        "\n",
        "    prec =  precision_score(real_membership_labels, attack_guesses)\n",
        "    print('Precision: {}'.format(prec))\n",
        "\n",
        "    recall = recall_score(real_membership_labels, attack_guesses)\n",
        "    print('Recall: {}'.format(recall))\n",
        "\n",
        "    fscore = f1_score(real_membership_labels, attack_guesses)\n",
        "    print('F1-Score: {}'.format(fscore))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # parameters\n",
        "    dpsgd = True\n",
        "\n",
        "    # target model hyper-parameters same as Lasso-dp\n",
        "    epochs = 10\n",
        "    batch_size = 32\n",
        "    microbatches_perc = .5\n",
        "    learning_rate = 0.01\n",
        "    kernel_regularization = 1.2 #0.001352\n",
        "    noise_multiplier = 0.8\n",
        "    l2_norm_clip = 0.8 #1.0\n",
        "\n",
        "    print(\"Loading and splitting dataset\")\n",
        "\n",
        "    X, y = personal_income_classification()\n",
        "    target_X, target_y, shadow_X, shadow_y = split_to_be_divisible(X, y, 0.5, batch_size=80)\n",
        "\n",
        "\n",
        "    shadow_X, shadow_y, unused_X, unused_y = split_to_be_divisible(shadow_X,\n",
        "                                                                   shadow_y,\n",
        "                                                                   0.3,\n",
        "                                                                   batch_size)\n",
        "    feature_size = target_X.shape[1]\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FxLQ5XzSZiTF",
        "outputId": "c79b270f-e5b9-44ac-f8db-e86c42fa0d46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and splitting dataset\n",
            "(32560, 15)\n",
            "Index(['39', ' State-gov', ' 77516', ' Bachelors', ' 13', ' Never-married',\n",
            "       ' Adm-clerical', ' Not-in-family', ' White', ' Male', ' 2174', ' 0',\n",
            "       ' 40', ' United-States', ' <=50K'],\n",
            "      dtype='object')\n",
            "    Age  Workclass    fnlwgt  Education  Education-num  Marital-status  \\\n",
            "0  50.0        6.0   83311.0        9.0           13.0             2.0   \n",
            "1  38.0        4.0  215646.0       11.0            9.0             0.0   \n",
            "2  53.0        4.0  234721.0        1.0            7.0             2.0   \n",
            "3  28.0        4.0  338409.0        9.0           13.0             2.0   \n",
            "4  37.0        4.0  284582.0       12.0           14.0             2.0   \n",
            "\n",
            "   Occupation  Relationship  Race  Sex  Capital-gain  Capital-loss  \\\n",
            "0         4.0           0.0   4.0  1.0           0.0           0.0   \n",
            "1         6.0           1.0   4.0  1.0           0.0           0.0   \n",
            "2         6.0           0.0   2.0  1.0           0.0           0.0   \n",
            "3        10.0           5.0   2.0  0.0           0.0           0.0   \n",
            "4         4.0           5.0   4.0  0.0           0.0           0.0   \n",
            "\n",
            "   Hours-per-week  Native-country  Salary  \n",
            "0            13.0            39.0     0.0  \n",
            "1            40.0            39.0     0.0  \n",
            "2            40.0            39.0     0.0  \n",
            "3            40.0             5.0     0.0  \n",
            "4            40.0            39.0     0.0  \n",
            "True\n",
            "True\n",
            "Training the target model...\n",
            "True\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 328, in __call__\n        total_total_loss_mean_value = tf.add_n(total_loss_mean_values)\n\n    ValueError: Shapes must be equal rank, but are 1 and 0\n    \tFrom merging shape 0 with other shapes. for '{{node AddN}} = AddN[N=2, T=DT_FLOAT](sparse_categorical_crossentropy/weighted_loss/Mul, dense_25/kernel/Regularizer/mul)' with input shapes: [32], [].\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-430dd963118c>\u001b[0m in \u001b[0;36m<cell line: 244>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    267\u001b[0m                                                                    batch_size)\n\u001b[1;32m    268\u001b[0m     \u001b[0mfeature_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-48-430dd963118c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0mtm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     tm.fit(target_X_train,\n\u001b[0m\u001b[1;32m    193\u001b[0m            \u001b[0mtarget_y_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m            \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 328, in __call__\n        total_total_loss_mean_value = tf.add_n(total_loss_mean_values)\n\n    ValueError: Shapes must be equal rank, but are 1 and 0\n    \tFrom merging shape 0 with other shapes. for '{{node AddN}} = AddN[N=2, T=DT_FLOAT](sparse_categorical_crossentropy/weighted_loss/Mul, dense_25/kernel/Regularizer/mul)' with input shapes: [32], [].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer\n",
        "from tensorflow.keras.layers import Dense, Input, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.regularizers import l1\n",
        "from mia.estimators import ShadowModelBundle, prepare_attack_data\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(19122)\n",
        "\n",
        "GradientDescentOptimizer = tf.compat.v1.train.GradientDescentOptimizer\n",
        "\n",
        "def split_to_be_divisible(X, y, shadow_perc, batch_size):\n",
        "    \"\"\"\n",
        "    Split a dataframe into target dataset and shadow dataset, and make them divisible by batch size.\n",
        "\n",
        "    :param X: genotype data\n",
        "    :param y: phenotype data\n",
        "    :param shadow_perc: specified percent for shadow dataset, target_perc = 1 - shadow_perc\n",
        "    :param batch_size: batch_size for training process\n",
        "\n",
        "    :return: target datasets, shadow datasets\n",
        "    \"\"\"\n",
        "\n",
        "    assert y.shape[0] == X.shape[0]\n",
        "\n",
        "    total_row = X.shape[0]\n",
        "    num_shadow_row = int(total_row * shadow_perc) - int(total_row * shadow_perc) % batch_size\n",
        "    num_target_row = (total_row - num_shadow_row) - (total_row - num_shadow_row) % batch_size\n",
        "\n",
        "    random_row = np.random.permutation(total_row)\n",
        "    shadow_row = random_row[:num_shadow_row]\n",
        "    target_row = random_row[-num_target_row:]\n",
        "\n",
        "    target_X = X.iloc[target_row]\n",
        "    shadow_X = X.iloc[shadow_row]\n",
        "\n",
        "    target_y = y.iloc[target_row]\n",
        "    shadow_y = y.iloc[shadow_row]\n",
        "\n",
        "    return target_X, target_y, shadow_X, shadow_y\n",
        "\n",
        "\n",
        "def target_model():\n",
        "    model = Sequential([\n",
        "        Input(shape=(feature_size,), name='feature'),\n",
        "        Flatten(),\n",
        "        Dense(128, activation=tf.nn.relu),\n",
        "        Dense(2, activation=tf.nn.softmax, kernel_regularizer=l1(kernel_regularization))\n",
        "    ])\n",
        "\n",
        "    if dpsgd:\n",
        "        optimizer = DPGradientDescentGaussianOptimizer(\n",
        "            l2_norm_clip=l2_norm_clip,\n",
        "            noise_multiplier=noise_multiplier,\n",
        "            num_microbatches=int(microbatches_perc * batch_size),\n",
        "            learning_rate=learning_rate)\n",
        "\n",
        "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.compat.v2.losses.Reduction.NONE)\n",
        "    else:\n",
        "        optimizer = GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.compat.v2.losses.Reduction.NONE)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def shadow_model():\n",
        "    model = Sequential([\n",
        "        Input(shape=(feature_size,), name='feature'),\n",
        "        Flatten(),\n",
        "        Dense(128, activation=tf.nn.relu),\n",
        "        Dense(2, activation=tf.nn.softmax, kernel_regularizer=l1(kernel_regularization))\n",
        "    ])\n",
        "\n",
        "    if dpsgd:\n",
        "        optimizer = DPGradientDescentGaussianOptimizer(\n",
        "            l2_norm_clip=l2_norm_clip,\n",
        "            noise_multiplier=noise_multiplier,\n",
        "            num_microbatches=int(microbatches_perc * batch_size),\n",
        "            learning_rate=learning_rate)\n",
        "\n",
        "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.compat.v2.losses.Reduction.NONE)\n",
        "    else:\n",
        "        optimizer = GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.compat.v2.losses.Reduction.NONE)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def personal_income_classification():\n",
        "    df = pd.read_csv(path + \"adult.data\", sep=\",\", engine=\"python\")\n",
        "\n",
        "    df.columns = ['Age', 'Workclass', 'fnlwgt', 'Education', 'Education-num', 'Marital-status', 'Occupation',\n",
        "                  'Relationship', 'Race', 'Sex', 'Capital-gain', 'Capital-loss',\n",
        "                  'Hours-per-week', 'Native-country', 'Salary']\n",
        "\n",
        "    df['Age'] = df['Age'].astype(np.float32)\n",
        "    df['fnlwgt'] = df['fnlwgt'].astype(np.float32)\n",
        "    df['Education-num'] = df['Education-num'].astype(np.float32)\n",
        "    df['Capital-gain'] = df['Capital-gain'].astype(np.float32)\n",
        "    df['Capital-loss'] = df['Capital-loss'].astype(np.float32)\n",
        "    df['Hours-per-week'] = df['Hours-per-week'].astype(np.float32)\n",
        "\n",
        "    categorical_columns = ['Workclass', 'Education', 'Marital-status', 'Occupation', 'Relationship', 'Race', 'Sex', 'Native-country', 'Salary']\n",
        "    for col in categorical_columns:\n",
        "        df[col] = df[col].astype('category').cat.codes.astype(np.float32)\n",
        "\n",
        "    df_train = df[['Age', 'Workclass', 'fnlwgt', 'Education', 'Education-num', 'Marital-status', 'Occupation', 'Relationship',\n",
        "                   'Race', 'Sex', 'Capital-gain', 'Capital-loss', 'Hours-per-week', 'Native-country']]\n",
        "\n",
        "    df_test = df[['Salary']]\n",
        "\n",
        "    return df_train, df_test\n",
        "\n",
        "\n",
        "def main():\n",
        "    # print(\"Training the target model...\")\n",
        "\n",
        "    # target_X_train, target_y_train, target_X_valid, target_y_valid = split_to_be_divisible(target_X, target_y, 0.2, batch_size)\n",
        "    # target_y_train_int = target_y_train.astype(int)\n",
        "    # target_y_valid_int = target_y_valid.astype(int)\n",
        "\n",
        "    # tm = target_model()\n",
        "    # tm.fit(target_X_train.values, target_y_train_int.values, batch_size=batch_size, epochs=epochs,\n",
        "    #        validation_data=(target_X_valid.values, target_y_valid_int.values), verbose=1)\n",
        "\n",
        "    print(\"Training the shadow model...\")\n",
        "\n",
        "    SHADOW_DATASET_SIZE = int(shadow_X.shape[0] / 2)\n",
        "    smb = ShadowModelBundle(\n",
        "        shadow_model,\n",
        "        shadow_dataset_size=SHADOW_DATASET_SIZE,\n",
        "        num_models=1,\n",
        "    )\n",
        "\n",
        "    attacker_X, attacker_y = smb.fit_transform(shadow_X.values, shadow_y.values,\n",
        "                                               fit_kwargs=dict(epochs=epochs,\n",
        "                                                               batch_size=batch_size,\n",
        "                                                               verbose=1))\n",
        "\n",
        "    print(\"Training the attack model...\")\n",
        "\n",
        "    clf = RandomForestClassifier(max_depth=2)\n",
        "    clf.fit(attacker_X, attacker_y)\n",
        "\n",
        "    ATTACK_TEST_DATASET_SIZE = unused_X.shape[0]\n",
        "    data_in = target_X_train[:ATTACK_TEST_DATASET_SIZE], target_y_train[:ATTACK_TEST_DATASET_SIZE]\n",
        "    data_out = unused_X[:ATTACK_TEST_DATASET_SIZE], unused_y[:ATTACK_TEST_DATASET_SIZE]\n",
        "\n",
        "    attack_test_data, real_membership_labels = prepare_attack_data(tm, data_in, data_out)\n",
        "\n",
        "    attack_guesses = clf.predict(attack_test_data)\n",
        "    attack_accuracy = accuracy_score(real_membership_labels, attack_guesses)\n",
        "    print('Attack accuracy: {}'.format(attack_accuracy))\n",
        "\n",
        "    prec = precision_score(real_membership_labels, attack_guesses)\n",
        "    print('Precision: {}'.format(prec))\n",
        "\n",
        "    recall = recall_score(real_membership_labels, attack_guesses)\n",
        "    print('Recall: {}'.format(recall))\n",
        "\n",
        "    fscore = f1_score(real_membership_labels, attack_guesses)\n",
        "    print('F1-Score: {}'.format(fscore))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # parameters\n",
        "    dpsgd = True\n",
        "    epochs = 10\n",
        "    batch_size = 32\n",
        "    microbatches_perc = 0.5\n",
        "    learning_rate = 0.01\n",
        "    kernel_regularization = 1.2\n",
        "    noise_multiplier = 0.8\n",
        "    l2_norm_clip = 0.8\n",
        "\n",
        "    path = \"./\"  # Specify the correct path for your dataset\n",
        "\n",
        "    print(\"Loading and splitting dataset\")\n",
        "    X, y = personal_income_classification()\n",
        "    target_X, target_y, shadow_X, shadow_y = split_to_be_divisible(X, y, 0.5, batch_size=80)\n",
        "\n",
        "    shadow_X, shadow_y, unused_X, unused_y = split_to_be_divisible(shadow_X, shadow_y, 0.3, batch_size)\n",
        "    feature_size = target_X.shape[1]\n",
        "\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "id": "zmMNJiLpea7c",
        "outputId": "0f216e7c-08c5-4325-dae4-b1368187d416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and splitting dataset\n",
            "Training the shadow model...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 328, in __call__\n        total_total_loss_mean_value = tf.add_n(total_loss_mean_values)\n\n    ValueError: Shapes must be equal rank, but are 1 and 0\n    \tFrom merging shape 0 with other shapes. for '{{node AddN}} = AddN[N=2, T=DT_FLOAT](sparse_categorical_crossentropy/weighted_loss/Mul, dense_29/kernel/Regularizer/mul)' with input shapes: [?], [].\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-3fa1f7b7729c>\u001b[0m in \u001b[0;36m<cell line: 173>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0mfeature_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-50-3fa1f7b7729c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m     )\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     attacker_X, attacker_y = smb.fit_transform(shadow_X.values, shadow_y.values,\n\u001b[0m\u001b[1;32m    144\u001b[0m                                                fit_kwargs=dict(epochs=epochs,\n\u001b[1;32m    145\u001b[0m                                                                \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mia/estimators.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, verbose, fit_kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mvalidation\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDoing\u001b[0m \u001b[0mso\u001b[0m \u001b[0mmay\u001b[0m \u001b[0mdecrease\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msuccess\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mattack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \"\"\"\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mia/estimators.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, verbose, pseudo, fit_kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;31m# Train the shadow model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mshadow_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mshadow_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mShadowModelBundle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL_ID_FMT\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshadow_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 328, in __call__\n        total_total_loss_mean_value = tf.add_n(total_loss_mean_values)\n\n    ValueError: Shapes must be equal rank, but are 1 and 0\n    \tFrom merging shape 0 with other shapes. for '{{node AddN}} = AddN[N=2, T=DT_FLOAT](sparse_categorical_crossentropy/weighted_loss/Mul, dense_29/kernel/Regularizer/mul)' with input shapes: [?], [].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from mia.estimators import ShadowModelBundle, prepare_attack_data\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Generate synthetic dataset for illustration\n",
        "# Replace this with your own dataset loading and preprocessing\n",
        "def generate_synthetic_data(num_samples=1000, sequence_length=10, num_features=5):\n",
        "    X = tf.random.normal((num_samples, sequence_length, num_features))\n",
        "    y = tf.concat([tf.zeros((num_samples // 2, 1)), tf.ones((num_samples // 2, 1))], axis=0)\n",
        "    return X, y\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = generate_synthetic_data()\n",
        "\n",
        "# Split data into training and non-training sets\n",
        "X_train, X_non_train, y_train, y_non_train = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define LSTM model\n",
        "model = Sequential([\n",
        "    LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the LSTM model on the training set\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Define shadow model architecture (same as target model)\n",
        "shadow_model = Sequential([\n",
        "    LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "shadow_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train shadow models on training and non-training sets\n",
        "shadow_X_train, _, shadow_y_train, _ = train_test_split(X_train, y_train, test_size=0.5, random_state=42)\n",
        "shadow_X_non_train, _, shadow_y_non_train, _ = train_test_split(X_non_train, y_non_train, test_size=0.5, random_state=42)\n",
        "\n",
        "smb = ShadowModelBundle(\n",
        "    lambda: shadow_model,\n",
        "    shadow_dataset_size=len(shadow_X_train),\n",
        "    num_models=5  # You can adjust the number of shadow models\n",
        ")\n",
        "\n",
        "# Fit and transform with shadow models\n",
        "attacker_X_train, attacker_y_train = smb.fit_transform(shadow_X_train, shadow_y_train, fit_kwargs=dict(epochs=5, batch_size=32, verbose=1))\n",
        "attacker_X_non_train, attacker_y_non_train = smb.fit_transform(shadow_X_non_train, shadow_y_non_train, fit_kwargs=dict(epochs=5, batch_size=32, verbose=1))\n",
        "\n",
        "# Combine shadow model data for attack model\n",
        "attacker_X = tf.concat([attacker_X_train, attacker_X_non_train], axis=0)\n",
        "attacker_y = tf.concat([tf.zeros_like(attacker_y_train), tf.ones_like(attacker_y_non_train)], axis=0)\n",
        "\n",
        "# Train an attack model (e.g., RandomForestClassifier)\n",
        "attack_model = RandomForestClassifier()\n",
        "attack_model.fit(attacker_X, attacker_y)\n",
        "\n",
        "# Convert indices to integers\n",
        "indices_train = random_row[:100].astype(int)\n",
        "indices_non_train = random_row[-100:].astype(int)\n",
        "\n",
        "# Test the success of the attack using the target model\n",
        "data_in = X_train[indices_train], y_train[indices_train]\n",
        "data_out = X_non_train[indices_non_train], y_non_train[indices_non_train]\n",
        "\n",
        "attack_test_data, real_membership_labels = prepare_attack_data(model, data_in, data_out)\n",
        "\n",
        "attack_guesses = attack_model.predict(attack_test_data)\n",
        "attack_accuracy = accuracy_score(real_membership_labels, attack_guesses)\n",
        "print('Attack accuracy: {}'.format(attack_accuracy))\n",
        "\n",
        "prec = precision_score(real_membership_labels, attack_guesses)\n",
        "print('Precision: {}'.format(prec))\n",
        "\n",
        "recall = recall_score(real_membership_labels, attack_guesses)\n",
        "print('Recall: {}'.format(recall))\n",
        "\n",
        "fscore = f1_score(real_membership_labels, attack_guesses)\n",
        "print('F1-Score: {}'.format(fscore))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JrrEg0Mwe7eE",
        "outputId": "89f950e7-5fc2-43e4-de42-0ffb9203d0a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got array([ 29, 535, 695, 557, 836, 596, 165, 918, 495, 824,  65, 141, 925,\n       827, 655, 331, 664, 249, 907, 708, 305, 734, 975,  49, 896,   2,\n       544, 350, 904, 536, 344, 994, 481, 575,  33,  31, 231, 963, 192,\n       333,   3, 204, 514, 799, 306, 109, 430,  77,  84, 286,  82, 991,\n       789, 894, 398, 323, 519, 916, 922,   5, 731, 465,  97, 266, 357,\n       868, 798, 380, 631, 381, 490, 118, 900, 250, 523,   9, 196, 603,\n        81, 783, 587, 797, 239, 290, 211, 717, 359, 449, 227, 950, 946,\n       796, 501, 464, 362, 468, 935, 428,   7, 155, 541, 440, 482, 422,\n       778, 949, 334, 576, 934, 567, 594, 530, 581, 707, 448, 453, 228,\n       352, 728, 212,  79, 148, 302, 628, 777, 506, 342, 485, 711, 133,\n       703, 311, 722, 629,   0, 316, 706, 547, 872, 532, 477, 404, 172,\n       125, 394, 420, 552, 903,  90, 939, 181, 274, 895,  69, 291, 131,\n       300, 424, 326, 144, 423, 580, 135, 450, 164,  28, 773, 193, 388,\n       852, 169, 705, 140, 173,   6, 745, 478,  73, 910, 813, 238, 145,\n       792, 234, 220, 923, 500, 132, 990, 774, 185,  41, 696, 108, 588,\n        56, 405, 442, 757, 997,  24, 467, 539, 531, 618, 694, 926, 338,\n        51, 507, 516, 920, 781, 264, 817, 710, 682, 832, 518, 447,  18,\n       715, 483, 568, 433, 367,  83,  61, 638, 272, 285, 360, 354, 456,\n       278,  12, 182, 368, 881, 615, 223, 572, 970, 653, 545, 582, 633,\n       176, 665, 673, 585, 873, 393, 163, 248, 634, 885, 669, 375, 412,\n        74, 113, 598, 961, 390, 104, 114, 417, 525, 457, 409,  92, 930,\n        89, 336, 988, 921, 933, 605, 593, 611,  94,  11, 396, 533,  43,\n        42, 329, 167, 497, 876, 597, 756, 100, 426, 178, 444, 416, 870,\n       882, 680, 177, 395, 911, 793, 960, 684, 383, 956, 751, 257, 538,\n       335,  15, 324, 758, 222, 179, 983,  22, 356, 666, 861, 340, 431,\n       551, 833, 203, 630,  93, 558,  68, 622, 284, 844, 434, 153,  75,\n       730, 446, 188, 271, 236, 487, 117, 943, 512, 825, 591, 126, 116,\n       473, 693,  57, 863, 912, 369, 268,  46, 349, 195, 999, 834, 736,\n       263, 443, 675, 304, 341, 966, 149, 124, 786,  50, 353, 927, 142,\n       470, 399, 625, 320,  19, 809, 790, 808, 407, 537, 620,  38, 175,\n       245, 828, 667, 754, 858, 154, 287, 602, 569, 743,  17, 127, 322,\n       255, 657, 964, 190, 115, 616, 606, 180, 301, 759, 712, 723, 685,\n       979, 517, 984,  45, 909, 157, 851, 171,  16, 511,  48, 971, 940,\n       515, 952, 480, 283, 718, 877, 225,  26, 954, 437, 951, 364, 229,\n        37, 965, 374, 469, 967, 850, 704, 841, 194, 854, 864, 503, 969,\n       830, 579, 968, 162, 908, 152, 801, 993, 755, 111, 226, 688, 103,\n       421, 419, 750, 586, 780, 672, 119,  53, 151, 403, 945, 207, 658,\n       843, 762,   8, 807,  36, 452, 651, 253, 303, 746, 571, 623, 732,\n       891, 262, 610, 297, 414, 150, 788, 640, 889, 550, 886, 488, 147,\n       146, 720, 931, 739, 659, 348, 463, 325, 186, 123, 853, 608, 143,\n       958, 197, 609, 279, 293, 400, 122, 183, 202, 438, 246, 415, 932,\n       765, 906, 835, 887, 129, 637, 402, 784, 770, 735, 913, 219, 641,\n       915, 752, 806, 919, 624, 874, 760, 386, 972, 509, 267, 819, 441,\n       496, 112, 691, 232, 869, 607, 671, 373, 981, 842, 233, 785, 676,\n       317, 648, 410, 898, 709, 358, 258, 744, 627, 632, 282, 376, 384,\n       224, 953, 814, 472, 347, 505, 639, 987, 928, 905, 619, 855, 803,\n       645, 846, 556, 957, 577, 795,  85, 242, 698, 159, 524,  35, 540,\n       170, 654, 890, 857, 847, 944, 733,  95, 563, 240, 742, 574, 690,\n       460, 553, 888, 206, 392, 794, 397, 766, 848, 217,   4, 768, 642,\n       929, 612, 738, 546, 725, 683,  98, 804, 727, 573, 406, 502,  47,\n        32, 779, 839, 200, 134,  27, 880, 230, 489, 772, 378, 288, 418,\n       674, 391, 592, 498, 138,  62, 471, 647, 128, 976, 520, 838, 962,\n        64, 812,  14, 156,  40, 492, 379, 187, 763, 216, 791,  52, 878,\n       337, 748, 719, 724, 295, 701, 251, 726, 461, 455, 996, 815, 862,\n       269, 201, 161, 555, 729, 401, 702, 476, 821, 771, 105, 565, 389,\n         1, 937, 982, 561,  80, 205,  34, 775, 508, 427, 454, 366,  91,\n       339, 897, 564, 345, 776, 241,  13, 315, 600, 387, 273, 166, 840,\n       992, 646, 818, 484, 980, 504, 831, 243, 566, 875, 562, 686, 189,\n       782, 699, 475, 681, 510,  58, 474, 560, 856, 747, 252,  21, 313,\n       459, 160, 276, 955, 191, 385, 805, 413, 491, 343, 769, 308, 661,\n       130, 663, 871,  99, 372,  87, 458, 330, 214, 466, 121, 614,  20,\n       700,  71, 106, 270, 860, 435, 102])",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-dfb8d91fcc45>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Split data into training and non-training sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_non_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_non_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Define LSTM model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2583\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstratify\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2585\u001b[0;31m     return list(\n\u001b[0m\u001b[1;32m   2586\u001b[0m         chain.from_iterable(\n\u001b[1;32m   2587\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2585\u001b[0m     return list(\n\u001b[1;32m   2586\u001b[0m         chain.from_iterable(\n\u001b[0;32m-> 2587\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2588\u001b[0m         )\n\u001b[1;32m   2589\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_safe_indexing\u001b[0;34m(X, indices, axis)\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_pandas_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_array_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_list_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_array_indexing\u001b[0;34m(array, key, key_dtype, axis)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_check_index\u001b[0;34m(idx)\u001b[0m\n\u001b[1;32m    959\u001b[0m     \u001b[0;31m# TODO(slebedev): IndexError seems more appropriate here, but it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m     \u001b[0;31m# will break `_slice_helper` contract.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_SLICE_TYPE_ERROR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", got {!r}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got array([ 29, 535, 695, 557, 836, 596, 165, 918, 495, 824,  65, 141, 925,\n       827, 655, 331, 664, 249, 907, 708, 305, 734, 975,  49, 896,   2,\n       544, 350, 904, 536, 344, 994, 481, 575,  33,  31, 231, 963, 192,\n       333,   3, 204, 514, 799, 306, 109, 430,  77,  84, 286,  82, 991,\n       789, 894, 398, 323, 519, 916, 922,   5, 731, 465,  97, 266, 357,\n       868, 798, 380, 631, 381, 490, 118, 900, 250, 523,   9, 196, 603,\n        81, 783, 587, 797, 239, 290, 211, 717, 359, 449, 227, 950, 946,\n       796, 501, 464, 362, 468, 935, 428,   7, 155, 541, 440, 482, 422,\n       778, 949, 334, 576, 934, 567, 594, 530, 581, 707, 448, 453, 228,\n       352, 728, 212,  79, 148, 302, 628, 777, 506, 342, 485, 711, 133,\n       703, 311, 722, 629,   0, 316, 706, 547, 872, 532, 477, 404, 172,\n       125, 394, 420, 552, 903,  90, 939, 181, 274, 895,  69, 291, 131,\n       300, 424, 326, 144, 423, 580, 135, 450, 164,  28, 773, 193, 388,\n       852, 169, 705, 140, 173,   6, 745, 478,  73, 910, 813, 238, 145,\n       792, 234, 220, 923, 500, 132, 990, 774, 185,  41, 696, 108, 588,\n        56, 405, 442, 757, 997,  24, 467, 539, 531, 618, 694, 926, 338,\n        51, 507, 516, 920, 781, 264, 817, 710, 682, 832, 518, 447,  18,\n       715, 483, 568, 433, 367,  83,  61, 638, 272, 285, 360, 354, 456,\n       278,  12, 182, 368, 881, 615, 223, 572, 970, 653, 545, 582, 633,\n       176, 665, 673, 585, 873, 393, 163, 248, 634, 885, 669, 375, 412,\n        74, 113, 598, 961, 390, 104, 114, 417, 525, 457, 409,  92, 930,\n        89, 336, 988, 921, 933, 605, 593, 611,  94,  11, 396, 533,  43,\n        42, 329, 167, 497, 876, 597, 756, 100, 426, 178, 444, 416, 870,\n       882, 680, 177, 395, 911, 793, 960, 684, 383, 956, 751, 257, 538,\n       335,  15, 324, 758, 222, 179, 983,  22, 356, 666, 861, 340, 431,\n       551, 833, 203, 630,  93, 558,  68, 622, 284, 844, 434, 153,  75,\n       730, 446, 188, 271, 236, 487, 117, 943, 512, 825, 591, 126, 116,\n       473, 693,  57, 863, 912, 369, 268,  46, 349, 195, 999, 834, 736,\n       263, 443, 675, 304, 341, 966, 149, 124, 786,  50, 353, 927, 142,\n       470, 399, 625, 320,  19, 809, 790, 808, 407, 537, 620,  38, 175,\n       245, 828, 667, 754, 858, 154, 287, 602, 569, 743,  17, 127, 322,\n       255, 657, 964, 190, 115, 616, 606, 180, 301, 759, 712, 723, 685,\n       979, 517, 984,  45, 909, 157, 851, 171,  16, 511,  48, 971, 940,\n       515, 952, 480, 283, 718, 877, 225,  26, 954, 437, 951, 364, 229,\n        37, 965, 374, 469, 967, 850, 704, 841, 194, 854, 864, 503, 969,\n       830, 579, 968, 162, 908, 152, 801, 993, 755, 111, 226, 688, 103,\n       421, 419, 750, 586, 780, 672, 119,  53, 151, 403, 945, 207, 658,\n       843, 762,   8, 807,  36, 452, 651, 253, 303, 746, 571, 623, 732,\n       891, 262, 610, 297, 414, 150, 788, 640, 889, 550, 886, 488, 147,\n       146, 720, 931, 739, 659, 348, 463, 325, 186, 123, 853, 608, 143,\n       958, 197, 609, 279, 293, 400, 122, 183, 202, 438, 246, 415, 932,\n       765, 906, 835, 887, 129, 637, 402, 784, 770, 735, 913, 219, 641,\n       915, 752, 806, 919, 624, 874, 760, 386, 972, 509, 267, 819, 441,\n       496, 112, 691, 232, 869, 607, 671, 373, 981, 842, 233, 785, 676,\n       317, 648, 410, 898, 709, 358, 258, 744, 627, 632, 282, 376, 384,\n       224, 953, 814, 472, 347, 505, 639, 987, 928, 905, 619, 855, 803,\n       645, 846, 556, 957, 577, 795,  85, 242, 698, 159, 524,  35, 540,\n       170, 654, 890, 857, 847, 944, 733,  95, 563, 240, 742, 574, 690,\n       460, 553, 888, 206, 392, 794, 397, 766, 848, 217,   4, 768, 642,\n       929, 612, 738, 546, 725, 683,  98, 804, 727, 573, 406, 502,  47,\n        32, 779, 839, 200, 134,  27, 880, 230, 489, 772, 378, 288, 418,\n       674, 391, 592, 498, 138,  62, 471, 647, 128, 976, 520, 838, 962,\n        64, 812,  14, 156,  40, 492, 379, 187, 763, 216, 791,  52, 878,\n       337, 748, 719, 724, 295, 701, 251, 726, 461, 455, 996, 815, 862,\n       269, 201, 161, 555, 729, 401, 702, 476, 821, 771, 105, 565, 389,\n         1, 937, 982, 561,  80, 205,  34, 775, 508, 427, 454, 366,  91,\n       339, 897, 564, 345, 776, 241,  13, 315, 600, 387, 273, 166, 840,\n       992, 646, 818, 484, 980, 504, 831, 243, 566, 875, 562, 686, 189,\n       782, 699, 475, 681, 510,  58, 474, 560, 856, 747, 252,  21, 313,\n       459, 160, 276, 955, 191, 385, 805, 413, 491, 343, 769, 308, 661,\n       130, 663, 871,  99, 372,  87, 458, 330, 214, 466, 121, 614,  20,\n       700,  71, 106, 270, 860, 435, 102])"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, Masking\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate some example data\n",
        "np.random.seed(42)\n",
        "seq_length = 10\n",
        "num_samples = 1000\n",
        "num_classes = 2\n",
        "\n",
        "X = np.random.randint(0, 2, size=(num_samples, seq_length))\n",
        "y = np.random.randint(0, num_classes, size=(num_samples,))\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_one_hot = to_categorical(y, num_classes=num_classes)\n",
        "\n",
        "# Split data into target and shadow sets\n",
        "X_target, X_shadow, y_target, y_shadow = train_test_split(X, y_one_hot, test_size=0.5, random_state=42)\n",
        "\n",
        "# Define LSTM model architecture\n",
        "model = Sequential([\n",
        "    Masking(mask_value=0, input_shape=(seq_length,)),\n",
        "    Embedding(input_dim=2, output_dim=16),\n",
        "    LSTM(32),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile and train the target LSTM model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_target, y_target, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Train a shadow LSTM model\n",
        "shadow_model = Sequential([\n",
        "    Masking(mask_value=0, input_shape=(seq_length,)),\n",
        "    Embedding(input_dim=2, output_dim=16),\n",
        "    LSTM(32),\n",
        "    Dense(1, activation='sigmoid')  # Binary classification: in or out of the training set\n",
        "])\n",
        "\n",
        "shadow_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "shadow_model.fit(X_shadow, np.ones(X_shadow.shape[0]), epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Create attack data (combine target and shadow data)\n",
        "attack_data = np.vstack((X_target, X_shadow))\n",
        "attack_labels = np.concatenate((np.ones(X_target.shape[0]), np.zeros(X_shadow.shape[0])))\n",
        "\n",
        "# Train an attack model\n",
        "attack_model = Sequential([\n",
        "    Masking(mask_value=0, input_shape=(seq_length,)),\n",
        "    Embedding(input_dim=2, output_dim=16),\n",
        "    LSTM(32),\n",
        "    Dense(1, activation='sigmoid')  # Binary classification: target or shadow\n",
        "])\n",
        "\n",
        "attack_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "attack_model.fit(attack_data, attack_labels, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate attack model\n",
        "attack_predictions = attack_model.predict(attack_data)\n",
        "attack_accuracy = np.mean((attack_predictions > 0.5) == attack_labels.reshape(-1, 1))\n",
        "print(f\"Attack accuracy: {attack_accuracy}\")\n"
      ],
      "metadata": {
        "id": "d9Sl0eRdgNvT",
        "outputId": "a2eab03b-5637-43a6-df81-967184fa7c86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "13/13 [==============================] - 6s 125ms/step - loss: 0.6935 - accuracy: 0.4850 - val_loss: 0.6920 - val_accuracy: 0.5000\n",
            "Epoch 2/10\n",
            "13/13 [==============================] - 0s 26ms/step - loss: 0.6924 - accuracy: 0.5175 - val_loss: 0.6914 - val_accuracy: 0.5000\n",
            "Epoch 3/10\n",
            "13/13 [==============================] - 0s 29ms/step - loss: 0.6924 - accuracy: 0.5175 - val_loss: 0.6909 - val_accuracy: 0.5000\n",
            "Epoch 4/10\n",
            "13/13 [==============================] - 0s 29ms/step - loss: 0.6921 - accuracy: 0.5175 - val_loss: 0.6897 - val_accuracy: 0.5000\n",
            "Epoch 5/10\n",
            "13/13 [==============================] - 0s 14ms/step - loss: 0.6924 - accuracy: 0.5250 - val_loss: 0.6887 - val_accuracy: 0.5300\n",
            "Epoch 6/10\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 0.6920 - accuracy: 0.5175 - val_loss: 0.6878 - val_accuracy: 0.5200\n",
            "Epoch 7/10\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 0.6919 - accuracy: 0.5175 - val_loss: 0.6871 - val_accuracy: 0.5200\n",
            "Epoch 8/10\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 0.6917 - accuracy: 0.5175 - val_loss: 0.6863 - val_accuracy: 0.5300\n",
            "Epoch 9/10\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 0.6920 - accuracy: 0.5200 - val_loss: 0.6859 - val_accuracy: 0.5300\n",
            "Epoch 10/10\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 0.6917 - accuracy: 0.5350 - val_loss: 0.6844 - val_accuracy: 0.5900\n",
            "Epoch 1/10\n",
            "13/13 [==============================] - 4s 53ms/step - loss: 0.6527 - accuracy: 1.0000 - val_loss: 0.6064 - val_accuracy: 1.0000\n",
            "Epoch 2/10\n",
            "13/13 [==============================] - 0s 11ms/step - loss: 0.5412 - accuracy: 1.0000 - val_loss: 0.4266 - val_accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 0.2672 - accuracy: 1.0000 - val_loss: 0.0743 - val_accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 0.0275 - accuracy: 1.0000 - val_loss: 0.0069 - val_accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "13/13 [==============================] - 0s 11ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "13/13 [==============================] - 0s 13ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "13/13 [==============================] - 0s 12ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "13/13 [==============================] - 0s 12ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "13/13 [==============================] - 0s 11ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
            "Epoch 1/10\n",
            "25/25 [==============================] - 3s 31ms/step - loss: 0.6762 - accuracy: 0.6250 - val_loss: 0.9133 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.6667 - accuracy: 0.6250 - val_loss: 0.9299 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.6637 - accuracy: 0.6250 - val_loss: 0.9365 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/10\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.6639 - accuracy: 0.6250 - val_loss: 0.9625 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/10\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.6617 - accuracy: 0.6250 - val_loss: 0.9709 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/10\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.6613 - accuracy: 0.6250 - val_loss: 0.9956 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/10\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.6613 - accuracy: 0.6250 - val_loss: 0.9711 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/10\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.6607 - accuracy: 0.6250 - val_loss: 0.9975 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/10\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.6612 - accuracy: 0.6250 - val_loss: 0.9832 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/10\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.6607 - accuracy: 0.6250 - val_loss: 0.9867 - val_accuracy: 0.0000e+00\n",
            "32/32 [==============================] - 1s 2ms/step\n",
            "Attack accuracy: 0.5\n"
          ]
        }
      ]
    }
  ]
}